{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c266afb1",
   "metadata": {},
   "source": [
    "## Creating Multimodal Experiences with Multiple API Calls\n",
    "\n",
    "The real excitement in modern AI development often comes from combining multiple capabilities into cohesive experiences. A truly multimodal assistant doesn't just answer questions with text—it can generate images, produce speech, and integrate these different modalities into responses that feel natural and appropriate for the context.\n",
    "\n",
    "### Text-to-Speech for Voice Responses\n",
    "\n",
    "Adding voice capabilities transforms a text-based assistant into something that feels more human and accessible. OpenAI's text-to-speech models, accessible through their TTS API, provide natural-sounding voice synthesis across multiple voice options.\n",
    "\n",
    "The API structure is straightforward:\n",
    "\n",
    "```python\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=f\n",
    "    )\n",
    "```\n",
    "\n",
    "Voice selection matters more than you might initially think. Different voices convey different personalities and work better in different contexts. OpenAI provides several voices with names like \"alloy,\" \"echo,\" \"fable,\" \"onyx,\" \"nova,\" and \"shimmer.\" Experimenting with these helps you find the voice that best matches your application's personality and use case.\n",
    "\n",
    "The audio response comes back as binary data that you can save to a file, stream to a user, or embed in your user interface. Gradio makes this particularly easy, as it has built-in support for displaying audio players that let users listen to generated speech.\n",
    "\n",
    "Response times for text-to-speech are generally quite fast—usually completing in a few seconds even for longer passages. This makes voice responses practical for interactive applications where users expect relatively immediate feedback.\n",
    "\n",
    "### Orchestrating Multiple Modalities\n",
    "\n",
    "The real power emerges when you coordinate multiple API calls to create rich, multimodal responses. Consider a travel assistant that, when asked about a destination, not only provides ticket pricing information but also generates an inspirational image of that destination and reads the response aloud. This creates an immersive experience that engages multiple senses and provides information in formats that suit different user preferences.\n",
    "\n",
    "Implementing this requires thoughtful orchestration:\n",
    "\n",
    "1. The user asks about a destination\n",
    "2. Your system calls the language model with tools enabled\n",
    "3. The model requests a tool call to fetch pricing\n",
    "4. Your code executes the database query\n",
    "5. Your code calls the image generation API with the destination city\n",
    "6. Your code calls back to the language model with the pricing information\n",
    "7. The model generates a text response\n",
    "8. Your code passes that response to the text-to-speech API\n",
    "9. Your UI displays the text, shows the image, and provides an audio player\n",
    "\n",
    "This sequence involves at least four distinct API calls (language model for initial response, tool execution, language model for final response, and potentially image generation and TTS), each of which needs to complete successfully for the full experience to work. Robust error handling becomes essential—you need graceful fallbacks if image generation fails or if audio synthesis encounters problems.\n",
    "\n",
    "### Scaling to Client-Server Databases\n",
    "\n",
    "For applications with higher concurrency requirements, multiple application instances, or larger data volumes, transitioning to a client-server database like PostgreSQL, MySQL, or a managed cloud database service becomes appropriate. The principles remain the same—your tool functions query the database to retrieve information or perform updates—but the connection mechanism and configuration change.\n",
    "\n",
    "Modern application development often uses cloud-managed database services like AWS RDS, Google Cloud SQL, or serverless options like Supabase or PlanetScale. These services handle backup, scaling, and maintenance, letting you focus on application logic rather than database administration.\n",
    "\n",
    "When connecting to remote databases, consider connection pooling to manage resources efficiently. Tools like SQLAlchemy in Python provide robust connection pooling and ORM capabilities that simplify database interaction in production applications.\n",
    "\n",
    "### The Limitation of UI-Based History\n",
    "\n",
    "When Gradio manages conversation history through the UI, it only tracks what's visible in the interface. This creates a subtle but significant problem with tool calling: the tool call requests and tool results don't appear in the Gradio chat interface, so they don't get included in the history that Gradio passes to your callback function.\n",
    "\n",
    "For many simple cases, this works fine. The language model is sophisticated enough to infer from the conversation context that it must have called a tool to obtain certain information. However, this approach isn't reliable for complex interactions or when precise context matters.\n",
    "\n",
    "### Database-Backed Conversation History\n",
    "\n",
    "Production applications should store complete conversation history in a database. This means persisting every message—system prompts, user messages, assistant responses, tool call requests, and tool results—in structured storage.\n",
    "\n",
    "A simple schema might look like:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE conversation_messages (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    conversation_id TEXT,\n",
    "    role TEXT,\n",
    "    content TEXT,\n",
    "    tool_call_id TEXT NULL,\n",
    "    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    metadata JSON NULL\n",
    ")\n",
    "```\n",
    "\n",
    "Your callback functions then load the complete conversation history from the database, append the new user message, make any necessary API calls including tool executions, save the new messages to the database, and return only what should be displayed in the UI.\n",
    "\n",
    "This approach provides several benefits beyond correct tool call handling:\n",
    "\n",
    "- Complete conversation logs for debugging and analysis\n",
    "- Ability to resume conversations across sessions\n",
    "- Support for conversation branching (letting users go back and try different approaches)\n",
    "- Analytics on how users interact with your assistant\n",
    "- Audit trails for applications where compliance matters\n",
    "\n",
    "## Testing and Comparing Models\n",
    "\n",
    "As AI capabilities proliferate across multiple providers, selecting the right model for specific tasks becomes increasingly important. Different models exhibit different strengths—some excel at creative tasks, others at analytical reasoning, some work better with code while others handle conversational nuance more naturally.\n",
    "\n",
    "### Structured Testing Approaches\n",
    "\n",
    "Creating fair comparisons between models requires thoughtful test design. You need tasks that are specific enough to judge quality objectively, yet open-ended enough to reveal the models' capabilities and limitations.\n",
    "\n",
    "The SVG drawing exercise provides an excellent example. By asking models to generate SVG code (which is essentially XML text describing how to draw shapes and lines), you test their ability to understand spatial relationships, decompose a visual concept into geometric components, and generate structured output that follows a specific format. This differs fundamentally from asking models to use image generation APIs—it tests reasoning and composition rather than leveraging specialized diffusion models.\n",
    "\n",
    "When testing models, maintain consistency across tests:\n",
    "\n",
    "- Use identical prompts for each model\n",
    "- Run tests under similar conditions (same time of day, similar system load)\n",
    "- Consider multiple runs to account for temperature-based variation\n",
    "- Document not just the results but also response times and costs\n",
    "\n",
    "Cost and latency often matter as much as quality. A model that produces slightly better results but costs ten times more or takes three times longer might not be the right choice for your application. Balance these factors based on your specific use case.\n",
    "\n",
    "### Using Open Router for Model Access\n",
    "\n",
    "Open Router simplifies model comparison by providing a single API that routes requests to dozens of different models from various providers. Instead of managing API keys and different client libraries for OpenAI, Anthropic, Google, and others, you use one Open Router API key and specify which model you want in each request.\n",
    "\n",
    "This routing approach differs from an abstraction layer. Open Router doesn't try to hide differences between models or translate between incompatible formats. Instead, it routes your request to the appropriate provider while maintaining the standard OpenAI-compatible API format that most models now support.\n",
    "\n",
    "For testing and comparison purposes, this is invaluable. You can write a single function that accepts a model identifier and runs your test, then iterate through a list of models to compare results:\n",
    "\n",
    "```python\n",
    "models_to_test = [\n",
    "    \"openai/gpt-4\",\n",
    "    \"anthropic/claude-3-opus\",\n",
    "    \"google/gemini-pro-1.5\",\n",
    "    \"meta-llama/llama-3.1-70b-instruct\"\n",
    "]\n",
    "\n",
    "for model in models_to_test:\n",
    "    result = run_test(model, test_prompt)\n",
    "    analyze_result(result, model)\n",
    "```\n",
    "\n",
    "This systematic approach helps you make informed decisions about which models to use in production, potentially using different models for different parts of your application based on their specific strengths.\n",
    "\n",
    "## Production Considerations and Best Practices\n",
    "\n",
    "Building prototypes that demonstrate capability differs significantly from building production systems that need to run reliably, handle errors gracefully, and serve real users at scale. As you progress from experimentation to deployment, several considerations become critical.\n",
    "\n",
    "### Error Handling and Graceful Degradation\n",
    "\n",
    "Every API call can fail. Networks experience issues, services have outages, rate limits get exceeded, and individual requests can timeout. Production code needs explicit error handling for each external dependency:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    response = client.chat.completions.create(...)\n",
    "except openai.APIError as e:\n",
    "    # Handle API errors (500s, connection issues)\n",
    "    return fallback_response\n",
    "except openai.RateLimitError:\n",
    "    # Handle rate limiting\n",
    "    return \"System is experiencing high load, please try again\"\n",
    "except Exception as e:\n",
    "    # Catch-all for unexpected errors\n",
    "    log_error(e)\n",
    "    return \"An unexpected error occurred\"\n",
    "```\n",
    "\n",
    "Graceful degradation means providing the best possible experience even when some features fail. If image generation fails but text responses work, show the text response and log the image generation error rather than failing the entire request.\n",
    "\n",
    "### Rate Limiting and Cost Management\n",
    "\n",
    "API costs and rate limits become real constraints in production. Implement monitoring to track your API usage and costs. Set up alerts when usage approaches concerning thresholds. Consider implementing user-level rate limiting to prevent individual users from exhausting your resources.\n",
    "\n",
    "For expensive operations like image generation, consider requiring explicit user confirmation or limiting the number of times a user can request images within a time period. Clear UI messaging about these limits helps users understand the constraints.\n",
    "\n",
    "### Security and Data Privacy\n",
    "\n",
    "When your application handles user data or enables actions with real-world consequences, security becomes paramount. Key considerations include:\n",
    "\n",
    "- Never store API keys in code; use environment variables or secure configuration management\n",
    "- Implement proper authentication and authorization for user access\n",
    "- Sanitize and validate all user inputs before processing\n",
    "- Be cautious about what information you include in prompts—don't send sensitive data to external APIs unnecessarily\n",
    "- Consider the privacy implications of storing conversation history\n",
    "- Implement appropriate access controls for administrative functions\n",
    "\n",
    "For applications in regulated industries (healthcare, finance, etc.), additional compliance requirements around data handling, audit logging, and access controls will apply.\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "While prototypes can afford to make sequential API calls and wait for each to complete, production applications often benefit from parallelization. Using asynchronous programming patterns in Python (with `asyncio` and `async/await`), you can make multiple API calls concurrently, significantly reducing overall response time.\n",
    "\n",
    "For example, if you need to generate both an image and audio for a response, these operations can proceed in parallel rather than sequentially, roughly halving the total wait time.\n",
    "\n",
    "Caching frequently accessed data reduces database load and API costs. If many users ask about the same destinations, caching the results of image generation or pricing lookups can improve performance and reduce costs without sacrificing accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
