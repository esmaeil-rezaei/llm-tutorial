{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3636b7",
   "metadata": {},
   "source": [
    "# Understanding Tokenizers and Transformer Architecture\n",
    "\n",
    "The journey into open source language model development requires understanding two fundamental concepts that bridge human language and mathematical computation: tokenizers and model architecture. While high-level APIs like pipelines abstract these details away, grasping how text transforms into numbers and how those numbers flow through neural network layers unlocks the ability to debug issues, optimize performance, and eventually fine-tune models for specific applications.\n",
    "\n",
    "## The Tokenization Process\n",
    "\n",
    "Language models are mathematical constructs built from billions of numerical parameters. They cannot directly process words, sentences, or paragraphs. Every interaction with a language model begins with translation: converting human-readable text into sequences of numbers that mathematical operations can manipulate.\n",
    "\n",
    "This translation occurs in two distinct stages, though practitioners often conflate the terminology. Understanding the precise distinction helps clarify conversations about model behavior and performance characteristics.\n",
    "\n",
    "### From Text to Tokens\n",
    "\n",
    "The first stage breaks continuous text into discrete chunks called tokens. These chunks represent meaningful units that balance several competing concerns. Individual characters would require enormous sequence lengths to represent even short passages. Complete words would create an unwieldy vocabulary containing hundreds of thousands of entries, many appearing only rarely in training data.\n",
    "\n",
    "Tokens occupy a middle ground. Common words like \"the\" or \"and\" typically map to single tokens. Less frequent words split into multiple tokens based on common subword patterns. The word \"tokenization\" might decompose into \"token\" and \"ization\" as two separate tokens. Rare technical terms or proper names often fragment into even smaller pieces.\n",
    "\n",
    "This subword tokenization approach, pioneered by techniques like Byte-Pair Encoding, provides crucial advantages. The vocabulary remains manageable, typically containing between 30,000 and 150,000 distinct tokens depending on the model. Yet the system can represent any word in any language by combining tokens, even words that never appeared in training data. If the model encounters a novel pharmaceutical compound name, it breaks the word into constituent parts that appeared separately during training, enabling it to process the unfamiliar term without special handling.\n",
    "\n",
    "### From Tokens to Token IDs\n",
    "\n",
    "The second stage maps each token to a unique integer identifier, often called a token ID. These IDs serve as indices into the model's vocabulary, a comprehensive list of all possible tokens the model recognizes. The token representing \"the\" might map to ID 300, while \"tokenization\" maps to ID 15847. The specific numbers vary across different models and tokenizers.\n",
    "\n",
    "This numeric representation enables the mathematical operations that drive language models. Neural networks multiply matrices, compute dot products, and apply activation functions—all operations defined for numbers rather than text. Token IDs provide the numeric index of the token that itself is a vector of numbers.\n",
    "\n",
    "### Special Tokens and Their Purpose\n",
    "\n",
    "Beyond tokens representing actual words and subwords, tokenizers include special tokens that convey structural information about input sequences. These special tokens play crucial roles in helping models understand context and generate appropriate responses.\n",
    "\n",
    "A beginning-of-sequence token signals that a new prompt is starting. An end-of-sequence token indicates completion. In chat-oriented models, additional special tokens delineate system instructions from user messages from assistant responses. The Llama model family, for instance, uses tokens like `<|begin_of_text|>` and `<|start_header_id|>` to structure conversations.\n",
    "\n",
    "Understanding special tokens requires recognizing a fundamental truth about how language models learn: these tokens carry meaning only because training data used them consistently. No special logic in the neural network architecture interprets a beginning-of-sequence token differently than any other token. The model learned through billions of training examples that sequences starting with this particular token ID follow certain patterns, and it replicates those patterns in its predictions.\n",
    "\n",
    "Consider an analogy to traditional statistical modeling. If you train a credit risk model and always include a binary flag indicating whether an applicant owns their home, the model learns to weight that feature appropriately based on patterns in training data. Similarly, when a language model encounters the token ID representing `<|start_header_id|>`, it has learned from countless examples that tokens following this pattern typically represent role identifiers like \"system\" or \"user\" or \"assistant.\"\n",
    "\n",
    "## Tokenizer Implementation Details\n",
    "\n",
    "Different models employ different tokenization strategies, and understanding this variation helps explain why models sometimes produce unexpected outputs or consume varying amounts of computational resources for similar inputs.\n",
    "\n",
    "### Model-Specific Tokenizers\n",
    "\n",
    "Each model family typically defines its own tokenizer, optimized for the model's training data and intended use cases. Meta's Llama models use one tokenizer, Microsoft's Phi models use another, and Google's Gemma models use yet another. These differences arise from independent design decisions made during model development.\n",
    "\n",
    "Tokenizer variations manifest in several ways. The total vocabulary size differs—Llama 3.2 uses approximately 128,000 tokens while other models might use 50,000 or 200,000. The algorithm for splitting words into subword tokens varies. One tokenizer might represent \"hugging\" as a single token while another splits it into \"hug\" and \"ging.\" The set of special tokens and their meanings differs across models.\n",
    "\n",
    "These variations have minimal practical impact on model selection decisions. A model with fewer tokens per sentence doesn't necessarily offer better performance or lower costs. The quality of generated outputs depends far more on model architecture, training data quality, and fine-tuning approaches than on tokenization specifics. Practitioners should match tokenizers to their chosen models rather than selecting models based on tokenization characteristics.\n",
    "\n",
    "### Working with Tokenizers in Code\n",
    "\n",
    "The Hugging Face Transformers library provides a unified interface for working with different tokenizers through the `AutoTokenizer` class. This abstraction loads the appropriate tokenizer for any model automatically, handling variations transparently.\n",
    "\n",
    "Loading a tokenizer requires specifying the model identifier from the Hugging Face Hub. For Llama 3.2, the identifier is `\"meta-llama/Llama-3.2-1B-Instruct\"`. The library downloads tokenizer configuration and vocabulary files, then instantiates the correct tokenizer class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "LLAMA = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
    "PHI = \"microsoft/Phi-4-mini-instruct\"\n",
    "GEMMA = \"google/gemma-3-270m-it\"\n",
    "QWEN = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "DEEPSEEK = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEEPSEEK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde06ade",
   "metadata": {},
   "source": [
    "Once instantiated, the tokenizer provides several key methods. The `encode` method converts text into token IDs, returning a list of integers. The `decode` method performs the reverse operation, converting token IDs back into human-readable text. The `batch_decode` method processes multiple sequences efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1636b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I'm excited to show tokenizers in action\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(token_ids)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6f4ec",
   "metadata": {},
   "source": [
    "Notice that decoding includes special tokens that weren't in the original text. The tokenizer automatically prepends a beginning-of-text token during encoding, and this special token appears when decoding. This behavior reflects how models expect to receive input during inference.\n",
    "\n",
    "### Chat Templates\n",
    "\n",
    "Chat-oriented language models expect input formatted in specific ways that distinguish system instructions, user messages, and assistant responses. Rather than requiring developers to manually construct these formats for each model, the Hugging Face library provides chat templates that handle formatting automatically.\n",
    "\n",
    "Chat templates accept messages in a standardized format—a list of dictionaries where each dictionary contains a \"role\" and \"content\" field. This format matches the structure used by OpenAI's API, making code portable across different model backends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33247d94",
   "metadata": {},
   "source": [
    "The AI model doesn't actually \"know\" what a Python list or a dictionary is. It only understands one long string of text. However, if you just jam all the text together, the AI won't know where the System instructions end and where the User's question begins.\n",
    "\n",
    "```python\n",
    "chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "\n",
    "tokenizer.chat_template = chat_template\n",
    "```\n",
    "\n",
    "Usually, when you download a model from HuggingFace, the creator includes this template inside the files. However, some models (especially \"Base\" models or older ones) don't include it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3283b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "\n",
    "# tokenizer.chat_template = chat_template\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for drivers.\"}\n",
    "]\n",
    "\n",
    "formatted_input = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "formatted_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1d359",
   "metadata": {},
   "source": [
    "The `apply_chat_template` method transforms this structured representation into the specific token sequence the model expects. For Llama models, this includes special tokens like `<|start_header_id|>` surrounding role identifiers and `<|eot_id|>` marking message boundaries. Different models use different conventions, but the chat template abstraction handles these variations transparently.\n",
    "\n",
    "Understanding what chat templates produce clarifies a potential misconception about language models. Beginners sometimes imagine that models have internal structure specifically designed to handle system prompts, user messages, and assistant responses—as if the neural network contains separate processing pathways for different message types.\n",
    "\n",
    "In reality, language models process a single continuous sequence of tokens. All the structure conveyed by message roles gets encoded through special tokens and text formatting. The model learned during training that certain patterns of special tokens typically precede certain types of content, and it generates outputs consistent with those learned patterns. The neural network itself remains agnostic to the semantic meaning of these special tokens; statistical patterns in training data teach it how to respond when encountering them.\n",
    "\n",
    "## Transformer Architecture Fundamentals\n",
    "\n",
    "Having established how text becomes token and embedding vectors, we can examine how these numeric sequences flow through the layers of a transformer neural network. Understanding model architecture at this level helps diagnose performance issues, estimate computational requirements, and reason about why certain fine-tuning approaches work.\n",
    "\n",
    "### The High-Level Structure\n",
    "\n",
    "Every transformer-based language model follows a similar architectural pattern consisting of three major components: an embedding layer, a series of transformer blocks, and an output layer that produces predictions.\n",
    "\n",
    "**The embedding layer**, sometimes called the token embedding or encoder layer, serves as the model's interface with token IDs. This layer converts each token ID into a dense vector representation with hundreds or thousands of dimensions. For Llama 3.2, each token maps to a 2,048-dimensional vector. These vectors capture semantic and syntactic properties of tokens in a format suitable for subsequent processing.\n",
    "\n",
    "The embedding process resembles looking up entries in a massive table. The model maintains a matrix with one row for each possible token ID and one column for each embedding dimension. Given token ID 300, the embedding layer retrieves row 300 from this matrix, producing a 2,048-dimensional vector. These vector representations, often called embeddings, encode the model's understanding of what each token means based on patterns observed during training.\n",
    "\n",
    "Following the embedding layer, transformer blocks process these vector sequences through multiple layers of transformations. Llama 3.2 with 1 billion parameters contains 16 transformer blocks stacked sequentially. Larger models include more blocks—Llama 3.1 with 8 billion parameters uses 32 blocks. Each block applies identical operations, though the learned parameters within each block differ.\n",
    "\n",
    "The final component, called the language modeling head, transforms the processed vectors back into a vocabulary-sized output. If the model has 128,256 possible tokens, the language modeling head produces 128,256 numbers for each position in the sequence. These numbers represent the model's confidence that each token should appear next. Selecting the token with the highest confidence produces the model's prediction.\n",
    "\n",
    "### Inside Transformer Blocks\n",
    "\n",
    "Each transformer block contains two primary sublayers: a multi-head self-attention mechanism and a feed-forward network, along with normalization operations that stabilize numerical computations.\n",
    "\n",
    "The self-attention mechanism implements the core innovation that made transformers dramatically more effective than previous architectures. This mechanism allows each position in the sequence to incorporate information from every other position, enabling the model to capture long-range dependencies and contextual relationships.\n",
    "\n",
    "Self-attention operates through learned projections called query, key, value, and output. For each position, the model computes a query vector representing what that position is looking for, key vectors for every position representing what information they contain, and value vectors containing the actual information to propagate. The query at one position scores against keys from all positions, producing attention weights that determine how much each value contributes to the output.\n",
    "\n",
    "This mechanism enables contextually appropriate behavior. When processing \"The bank was steep,\" the word \"bank\" attends strongly to \"steep,\" helping disambiguate toward the riverbank meaning rather than the financial institution meaning. The model learned these attention patterns from training data without explicit supervision about word meanings or grammatical relationships.\n",
    "\n",
    "For more details and the mathematical foundations of transformers, refer to [this repository](https://github.com/esmaeil-rezaei/transformers-math).\n",
    "\n",
    "### Quantization: Trading Precision for Efficiency\n",
    "\n",
    "Neural network parameters are typically stored as 16-bit or 32-bit floating-point numbers, providing high precision and large dynamic range. However, this precision comes at a cost: memory consumption and computational intensity both scale linearly with parameter bit width.\n",
    "\n",
    "**Quantization reduces parameter precision, storing weights as 8-bit or even 4-bit numbers instead**. This reduction decreases memory requirements proportionally—4-bit quantization uses one quarter the memory of 16-bit representations. Computational throughput also improves because operations on lower-precision numbers execute faster on modern hardware.\n",
    "\n",
    "The remarkable aspect of quantization is that reducing precision to 4 bits degrades model performance far less than might be expected. Intuitively, reducing each parameter's precision by 75% seems equivalent to removing 75% of parameters entirely. **Yet empirical results show that quantized models retain significantly more capability than pruned models with equal memory footprints**.\n",
    "\n",
    "Several factors explain this phenomenon. Neural networks exhibit substantial redundancy, with many parameters contributing similar information. Reducing precision forces the model to encode information more efficiently in its remaining representational capacity. Training procedures that anticipate quantization can optimize for robustness to precision reduction. The specific quantization schemes used, such as NF4 (Normal Float 4), map 4-bit values to floating-point ranges in ways that preserve the most informative distinctions while discarding less critical precision.\n",
    "\n",
    "From a practical perspective, quantization enables running larger models on hardware with limited memory. A 7-billion parameter model quantized to 4 bits requires approximately 3.5 gigabytes of GPU memory compared to 14 gigabytes at 16-bit precision. This reduction can mean the difference between a model fitting on consumer hardware or requiring expensive data center GPUs. We will discuss quantization in detail moving forward.\n",
    "\n",
    "## Practical Model Usage\n",
    "\n",
    "Theory provides foundation, but practical experience builds intuition. Working with actual models reveals how abstract concepts manifest in code and how different implementation choices affect results.\n",
    "\n",
    "### Loading and Configuring Models\n",
    "\n",
    "The Hugging Face Transformers library provides `AutoModelForCausalLM` for loading language models that generate text by predicting the next token. Similar to tokenizer loading, this class automatically selects the appropriate model architecture based on the model identifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b585d2",
   "metadata": {},
   "source": [
    "The `device_map=\"auto\"` parameter tells the library to automatically place model components on available hardware, preferring GPUs when available. The `torch_dtype` parameter specifies the numerical precision for model weights—16-bit floating point provides a good balance between memory efficiency and numerical stability.\n",
    "\n",
    "For quantized models, additional configuration specifies the quantization scheme. The `BitsAndBytesConfig` class from the `bitsandbytes` library configures 4-bit or 8-bit quantization with various options for quantization data types and double quantization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ba085",
   "metadata": {},
   "source": [
    "> Important note: bitsandbytes (the library behind BitsAndBytesConfig) is currently an NVIDIA-only tool. It requires a specific software layer called CUDA to perform 4-bit math. If you are on a Mac (which uses Apple's \"Metal\" or \"MPS\") or a system without an NVIDIA GPU, the library crashes because it cannot find its engine. However, the latest versions of bitsandbytes (v0.44+) have added experimental support for Mac, but it is very slow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc41c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEEPSEEK,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37585a24",
   "metadata": {},
   "source": [
    "Loading models from the Hugging Face Hub downloads potentially large files—several gigabytes for billion-parameter models. The first time you load a model, expect significant download time depending on your network connection. Subsequent loads use cached files from your local filesystem, dramatically reducing latency.\n",
    "\n",
    "### Generating Text\n",
    "\n",
    "With a model and tokenizer loaded, text generation follows a straightforward workflow. Format your input using the chat template, convert to token IDs, pass through the model, and decode the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ce7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a lighthearted joke for data scientists.\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=80,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b49203d",
   "metadata": {},
   "source": [
    "The `generate` method implements the iterative process of predicting one token at a time, appending each prediction to the input sequence, and repeating until reaching a stopping condition. The `max_new_tokens` parameter limits generation length to prevent runaway output. The `pad_token_id` parameter handles technical details of batched generation.\n",
    "\n",
    "Advanced generation parameters provide fine-grained control over output characteristics. Temperature scaling adjusts randomness in token selection. Top-k and top-p sampling constrain the set of tokens considered for selection. Repetition penalties discourage the model from generating repetitive sequences. These parameters enable tailoring generation behavior to specific use cases.\n",
    "\n",
    "### Streaming Responses\n",
    "\n",
    "For interactive applications, waiting for complete responses before displaying any text creates poor user experience. Streaming generation displays tokens as the model produces them, providing immediate feedback and reducing perceived latency.\n",
    "\n",
    "The Transformers library supports streaming through the `TextStreamer` class. This class receives tokens as generation proceeds and immediately decodes and prints them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca7af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=80,\n",
    "    streamer=streamer,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d735165",
   "metadata": {},
   "source": [
    "As generation proceeds, tokens appear in the output stream in real-time rather than waiting for the complete response. This approach dramatically improves user experience for long-form generation tasks.\n",
    "\n",
    "## Comparing Models\n",
    "\n",
    "Open source language models span a wide range of sizes, architectures, and capabilities. Hands-on comparison across different models builds intuition about the tradeoffs between model scale, computational requirements, and output quality.\n",
    "\n",
    "### Small vs. Large Models\n",
    "\n",
    "Model size, typically measured in billions of parameters, strongly correlates with capabilities. Larger models generally produce more coherent, contextually appropriate, and factually accurate outputs. However, larger models also require more memory and compute time.\n",
    "\n",
    "A 270-million parameter model like Gemma 3 Tiny occupies approximately 500 megabytes of memory when quantized and generates tokens quickly even on modest hardware. However, its outputs often lack coherence beyond a few sentences and struggle with tasks requiring deep reasoning or extensive world knowledge.\n",
    "\n",
    "A 4-billion parameter model like Phi-4 requires several gigabytes of memory but produces substantially more coherent and contextually appropriate responses. These models handle many practical tasks effectively while remaining accessible on consumer hardware.\n",
    "\n",
    "Models with 7-70 billion parameters offer capabilities approaching frontier models for many tasks but require high-end GPUs with substantial memory. These models represent the current sweet spot for many production applications, balancing capability with deployment feasibility.\n",
    "\n",
    "# NOTE\n",
    "\n",
    "We can only apply quantization to **open-source models** because this optimization requires direct access to the model's \"weights\"—the raw mathematical values that make up its brain—which the creators have made public. Conversely, we **cannot quantize proprietary models** (like GPT-4 or Claude) because they are kept behind a \"black box\" API; since we lack access to their internal weight files, we cannot modify their precision to lower costs or optimize them for our own hardware.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
