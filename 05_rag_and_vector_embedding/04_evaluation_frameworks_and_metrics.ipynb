{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0087778e",
   "metadata": {},
   "source": [
    "# Building Scientific Rigor Into RAG Systems: Evaluation Frameworks and Metrics\n",
    "\n",
    "## The Critical Importance of Evaluation in RAG Development\n",
    "\n",
    "Welcome to what may be the most crucial chapter in your entire journey toward mastering large language model engineering. In this notebook we confront a fundamental challenge that separates successful production systems from experimental prototypes: how do you measure whether your RAG system actually works?\n",
    "\n",
    "This question might seem straightforward at first glance, but it reveals layers of complexity that define the difference between engineering and guesswork. Building a RAG pipeline that appears to function takes mere minutes. Building one that reliably delivers accurate, complete, and relevant answers to real business questions requires a systematic evaluation framework.\n",
    "\n",
    "The concepts presented in this chapter apply universally across virtually any business domain. RAG represents one of the most immediately applicable LLM solutions available today, making evaluation skills directly transferable to real-world problems you face in your organization. As you progress through this material, continuously ask yourself: how would I apply these evaluation techniques to my specific business challenges?\n",
    "\n",
    "## The Paradox of RAG: Power and Peril\n",
    "\n",
    "Before diving into evaluation methodologies, we must honestly assess both the strengths and limitations of RAG systems. Understanding these trade-offs illuminates why rigorous evaluation becomes absolutely essential.\n",
    "\n",
    "### The Compelling Advantages of RAG\n",
    "\n",
    "RAG systems offer several powerful benefits that explain their widespread adoption. First and foremost, they enable remarkably rapid development cycles. You witnessed this yourself when building a functional question-answering system about insurance products in just minutes. This speed-to-market advantage stands in stark contrast to approaches like fine-tuning, which require substantial time investment, computational resources, and expertise.\n",
    "\n",
    "Scalability represents another major advantage. Your current implementation handles dozens of documents, but the same architectural patterns scale elegantly to hundreds of thousands or even millions of documents. The ingestion process might take longer initially, but once vectors populate your database, retrieval remains consistently fast regardless of knowledge base size.\n",
    "\n",
    "RAG systems also optimize context window usage brilliantly. Rather than stuffing entire document collections into every prompt—wasting tokens and incurring unnecessary costs—RAG selectively retrieves only the most relevant fragments. This surgical precision means you pay only for the context that actually matters for each specific query.\n",
    "\n",
    "Related to context efficiency, RAG prevents context pollution. When you include irrelevant information in a prompt, you risk confusing the language model or introducing noise that degrades response quality. RAG's targeted retrieval ensures that the model focuses exclusively on pertinent information, improving both accuracy and coherence.\n",
    "\n",
    "### The Uncomfortable Realities of RAG\n",
    "\n",
    "Now for the less glamorous truth: RAG is fundamentally a hack. This characterization might sound harsh, but understanding it proves essential for effective RAG engineering.\n",
    "\n",
    "Transformers were designed with attention mechanisms that learn to identify and focus on relevant information within their input. These neural architectures excel at determining what matters and what doesn't through layers of learned parameters. Our challenge arises when we have far more information than we can reasonably fit into the context window. Rather than trusting the transformer's sophisticated attention mechanisms, we've introduced an upstream filtering step that uses vector similarity as a proxy for relevance.\n",
    "\n",
    "This vector-based filtering operates on probabilities and approximations. We're essentially betting that chunks positioned closely in vector space probably contain relevant information, while distant chunks probably don't. Notice the repeated use of \"probably\"—this probabilistic nature means RAG sometimes surfaces incorrect context or misses crucial information.\n",
    "\n",
    "The empirical, trial-and-error nature of RAG optimization flows directly from this fundamental characteristic. Students frequently ask questions like: \"I have documents of this type with this structure, and users ask these kinds of questions—which encoder should I use? What chunk size is optimal? Which RAG variant should I implement?\" The honest answer is always: \"I can offer informed guesses based on experience, but you won't know until you test it systematically.\"\n",
    "\n",
    "This experimental requirement isn't a temporary limitation that better tools will eventually overcome. It's intrinsic to the approach. RAG optimizes for retrieval performance through heuristics rather than learned parameters, making empirical validation the only reliable path forward.\n",
    "\n",
    "### The Whack-a-Mole Problem\n",
    "\n",
    "Perhaps the most frustrating aspect of RAG development manifests as the whack-a-mole phenomenon. You optimize your system to correctly answer a question like \"Who won the prestigious award?\" by adjusting chunk sizes or overlap parameters. Success! The system now retrieves the right context and produces accurate answers. Then you discover that your changes broke three other questions that previously worked correctly.\n",
    "\n",
    "You address those three questions by modifying your encoder or retrieval strategy. Great! Those three now work. But your original question about the award winner starts returning \"I don't know\" again. Fix that, and two different questions start hallucinating. This cycle continues endlessly without a systematic evaluation framework to guide your decisions.\n",
    "\n",
    "The tyranny of RAG lies in this unpredictability. Brilliant ideas that seem destined to work sometimes fail spectacularly. Simple tweaks you implement as afterthoughts sometimes yield dramatic improvements. Intuition alone cannot navigate this landscape—you need quantitative metrics that reveal whether changes actually help or hurt overall system performance.\n",
    "\n",
    "## The Framework for Scientific RAG Development\n",
    "\n",
    "Evaluation transforms RAG development from alchemy into engineering. The process consists of three fundamental steps that provide structure to what otherwise feels like chaotic experimentation.\n",
    "\n",
    "### Step One: Curating Your Golden Dataset\n",
    "\n",
    "The foundation of any evaluation framework is a carefully constructed test dataset. This golden dataset consists of representative questions paired with metadata that enables automated evaluation. For each test case, you typically need:\n",
    "\n",
    "**The question itself**: A realistic query that your system should be able to answer based on your knowledge base. These questions should reflect actual user information needs, not contrived examples designed to showcase your system's strengths.\n",
    "\n",
    "**Keywords that should appear in relevant context**: Identify specific terms or phrases that retrieved chunks must contain to be considered relevant. For a question like \"Who won the prestigious Innovator of the Year award?\", your keywords might include \"Maxine\", \"Thompson\", and \"IoTY\" (the award abbreviation). This enables automated evaluation of retrieval quality without manual inspection of every chunk.\n",
    "\n",
    "**Reference answer**: A gold-standard response that represents the ideal answer to the question. This doesn't need to be the only possible correct answer, but it should be complete, accurate, and appropriately scoped. For our award question, a reference answer might be: \"Maxine Thompson won the prestigious Innovator of the Year award in 2023.\"\n",
    "\n",
    "**Category or classification**: Grouping test cases by type helps you understand where your system excels and where it struggles. Categories might include \"direct facts\", \"temporal queries\", \"numerical questions\", \"relationship questions\", \"spanning questions\" (requiring synthesis across multiple documents), and \"holistic questions\" (requiring understanding of the entire knowledge base).\n",
    "\n",
    "### Sources for Golden Dataset Creation\n",
    "\n",
    "Where do these test cases come from? You have several options, each with distinct trade-offs.\n",
    "\n",
    "The most straightforward approach involves manually creating questions by thoroughly reviewing your knowledge base. Identify the types of questions your system should answer, then craft representative examples with their corresponding keywords and reference answers. This manual curation gives you complete control over coverage and quality, but requires substantial time investment.\n",
    "\n",
    "A far superior source, when available, consists of real user questions and expert answers from production systems. If you're building a RAG system to replace or augment an existing process—perhaps automating responses to customer inquiries that currently require human experts—you already have a treasure trove of authentic questions and validated answers. This real-world data captures the actual information needs of your users, including edge cases and phrasings you might never consider when generating synthetic examples.\n",
    "\n",
    "Many practitioners leverage large language models to generate synthetic test cases. You can provide the LLM with your knowledge base documents and ask it to generate diverse, realistic questions covering different topics and difficulty levels. This approach scales efficiently, but requires careful review to ensure the generated questions are genuinely representative and that reference answers are accurate.\n",
    "\n",
    "### The Living Nature of Test Datasets\n",
    "\n",
    "Your golden dataset should evolve continuously rather than remaining static after initial creation. As you identify edge cases where your system fails, add those cases to your test suite. When users report incorrect answers or ask questions your system struggles with, incorporate those examples into your evaluation framework.\n",
    "\n",
    "This continuous expansion serves two purposes. First, it helps you avoid overfitting to your initial test set. If you optimize exclusively for 50 fixed questions, your system might excel at those specific queries while failing on slightly different phrasings or topics. A growing, diverse test set prevents this narrow optimization.\n",
    "\n",
    "Second, a living dataset helps you detect regressions. When you modify your chunking strategy or switch embedding models to improve performance on one category of questions, your expanded test set immediately reveals whether those changes inadvertently degraded performance elsewhere.\n",
    "\n",
    "## Evaluating Retrieval: Metrics That Matter\n",
    "\n",
    "With a golden dataset in hand, you can begin systematic evaluation. The first category of metrics focuses specifically on retrieval quality: how effectively does your vector search surface relevant context?\n",
    "\n",
    "### Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Mean Reciprocal Rank represents one of the most intuitive and widely-used retrieval metrics. The concept is elegantly simple: for each test question, determine the position of the first retrieved chunk that contains relevant information (matches your keywords). Convert that position to a reciprocal rank, then average across all test cases.\n",
    "\n",
    "If the first chunk contains relevant information, the reciprocal rank is $$\\frac{1}{1} = 1.0$$ (perfect). If the second chunk is the first to contain relevant information, the reciprocal rank is $$\\frac{1}{2} = 0.5$$. The third chunk yields $$\\frac{1}{3} \\approx 0.33$$, and so on. Average these values across your entire test set to compute the mean reciprocal rank.\n",
    "\n",
    "An MRR of 1.0 indicates that relevant information always appears in the very first chunk retrieved—ideal performance. An MRR of 0.5 suggests that on average, you need to look at two chunks before finding relevant content. Lower values indicate increasingly poor retrieval performance.\n",
    "\n",
    "MRR's strength lies in its emphasis on the position of the first relevant result. Users typically care most about whether they get good information quickly, making this metric well-aligned with user experience concerns.\n",
    "\n",
    "### Normalized Discounted Cumulative Gain (NDCG)\n",
    "\n",
    "While MRR focuses on the first relevant result, NDCG evaluates the overall distribution of relevant content across all retrieved chunks. The metric asks: are all your relevant chunks concentrated at the top of your results, or are they scattered throughout?\n",
    "\n",
    "The formula incorporates logarithmic discounting based on rank position:\n",
    "\n",
    "$$\\text{DCG} = \\sum_{i=1}^{k} \\frac{\\text{relevance}_i}{\\log_2(i + 1)}$$\n",
    "\n",
    "The \"normalized\" aspect ensures that the metric accounts for the maximum possible DCG given your specific test case. If a question could only possibly have two relevant chunks in your entire knowledge base, you're not penalized for failing to surface a third relevant chunk (which doesn't exist). This normalization makes NDCG scores comparable across different queries with varying numbers of relevant results.\n",
    "\n",
    "Perfect NDCG scores of 1.0 occur when all relevant chunks appear at the very top of your retrieval results. Lower scores indicate that relevant content appears further down in the ranking or is mixed with less relevant chunks.\n",
    "\n",
    "NDCG provides richer information than MRR but is also more complex to interpret. Many practitioners find MRR sufficient for most RAG evaluation needs, resorting to NDCG primarily for research or highly optimized production systems.\n",
    "\n",
    "### Recall at K\n",
    "\n",
    "Recall represents a fundamental concept from information retrieval and machine learning. In the RAG context, recall at K measures: \"What percentage of my test cases have at least one relevant chunk within the top K retrieved results?\"\n",
    "\n",
    "For example, recall at 5 means: if you retrieve 5 chunks for each query, what percentage of your queries successfully retrieved at least one chunk containing relevant information? A recall at 5 of 0.80 indicates that 80% of your queries succeeded in surfacing relevant context within the top 5 chunks, while 20% failed to surface relevant information at all.\n",
    "\n",
    "You can extend this concept to keyword coverage when dealing with multiple required keywords per query. Keyword coverage measures what percentage of your total keywords appear somewhere within the retrieved chunks across all test cases. If you have 300 total keywords across 100 test questions, and your retrieval process finds 240 of them, your keyword coverage is 80%.\n",
    "\n",
    "Recall metrics directly measure whether your retrieval system successfully surfaces the information needed to answer questions. High recall is essential—if relevant context never makes it into the prompt, even the most sophisticated language model cannot produce accurate answers.\n",
    "\n",
    "### Precision at K\n",
    "\n",
    "Precision represents the flip side of recall, measuring: \"What proportion of retrieved chunks actually contain relevant information?\" If you retrieve 5 chunks per query and on average 3 of those 5 contain relevant content, your precision at 5 is 0.60.\n",
    "\n",
    "In RAG contexts, precision typically matters less than recall. Including some irrelevant chunks in your context won't necessarily prevent the language model from generating good answers—the model's attention mechanisms should learn to focus on what matters. However, very low precision indicates that you're wasting context window space on noise, potentially confusing the model or unnecessarily increasing costs.\n",
    "\n",
    "Precision becomes more important when dealing with strict context length limitations or when irrelevant information tends to actively mislead your language model. For most applications, prioritize optimizing recall and keyword coverage over precision.\n",
    "\n",
    "## Evaluating Answer Quality: LLM as Judge\n",
    "\n",
    "A practical implementation might use a structured prompt like:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ebadd5",
   "metadata": {},
   "source": [
    "```python\n",
    "prompt = f\"\"\"You are an expert evaluator assessing the quality of answers.\n",
    "\n",
    "Evaluate the generated answer by comparing it to the reference answer.\n",
    "Only give 5 out of 5 scores for perfect answers.\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {generated_answer}\n",
    "Reference Answer: {reference_answer}\n",
    "\n",
    "Please evaluate the generated answer on these dimensions:\n",
    "1. Accuracy (1-5): Is the information factually correct?\n",
    "2. Completeness (1-5): Does it include all essential information?\n",
    "3. Relevance (1-5): Does it stay focused without extraneous details?\n",
    "\n",
    "Provide your scores and brief feedback explaining your assessment.\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae665e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "\n",
    "documents = []\n",
    "folders = ['company', 'contracts', 'employees', 'products']\n",
    "\n",
    "for folder in folders:\n",
    "    doc_type = folder\n",
    "    loader = DirectoryLoader(\n",
    "        f'knowledge_base/{folder}',\n",
    "        glob='**/*.md',\n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs=text_loader_kwargs\n",
    "    )\n",
    "    folder_docs = loader.load()\n",
    "    \n",
    "    for doc in folder_docs:\n",
    "        doc.metadata['doc_type'] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb359e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=\"vector_db\")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a243212",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925dd5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a284fe",
   "metadata": {},
   "source": [
    "The judge model—often a capable model like GPT-4 or Claude—then generates structured feedback with numerical scores for each dimension. Using a more powerful model as judge generally produces more reliable evaluations, though for many use cases, lighter models like GPT-4o-mini provide sufficient quality at much lower cost.\n",
    "\n",
    "### Structured Outputs for Reliable Evaluation\n",
    "\n",
    "When implementing LLM as judge, leveraging structured outputs significantly improves reliability and parsing simplicity. Rather than hoping the model formats its response consistently, you explicitly specify the expected JSON schema using tools like Pydantic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872314c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "import numpy as np\n",
    "\n",
    "class AnswerEvaluation(BaseModel):\n",
    "    feedback: str = Field(description=\"Explanation of the evaluation\")\n",
    "    accuracy: int = Field(description=\"Accuracy score from 1-5\")\n",
    "    completeness: int = Field(description=\"Completeness score from 1-5\")\n",
    "    relevance: int = Field(description=\"Relevance score from 1-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209248a0",
   "metadata": {},
   "source": [
    "By passing this schema to your LLM API call (supported by OpenAI, Anthropic, and other providers), you guarantee that responses conform to your expected structure. This eliminates parsing errors and ensures consistent evaluation across thousands of test cases.\n",
    "\n",
    "## The Complete Evaluation Workflow\n",
    "\n",
    "With both retrieval metrics and answer quality metrics defined, you can establish a comprehensive evaluation workflow that brings scientific rigor to RAG development.\n",
    "\n",
    "### Building the Test Infrastructure\n",
    "\n",
    "Start by encoding your golden dataset in a structured format. JSONL (JSON Lines) format works particularly well—each line contains a complete JSON object representing one test case:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44524ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"question\": \"Who won the prestigious award in 2023?\",\n",
    "    \"keywords\": [\"Maxine\", \"Thompson\", \"IoTY\"],\n",
    "    \"reference_answer\": \"Maxine Thompson won the prestigious Innovator of the Year award in 2023.\",\n",
    "    \"category\": \"direct_fact\"\n",
    "}\n",
    "{\n",
    "    \"question\": \"How long was Sarah Chen employed?\",\n",
    "    \"keywords\": [\"Sarah Chen\", \"2018\", \"2023\", \"five years\"],\n",
    "    \"reference_answer\": \"Sarah Chen was employed for five years, from 2018 to 2023.\",\n",
    "    \"category\": \"temporal\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f56a6",
   "metadata": {},
   "source": [
    "This format allows easy appending of new test cases and straightforward loading into Python data structures.\n",
    "\n",
    "### Automating the Evaluation Process\n",
    "\n",
    "Create modular evaluation functions that separate concerns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e93656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(retrieved_chunks, keywords):\n",
    "    for idx, chunk in enumerate(retrieved_chunks, start=1):\n",
    "        if any(keyword in chunk.page_content for keyword in keywords):\n",
    "            return 1 / idx\n",
    "    return 0.0\n",
    "\n",
    "def calculate_ndcg(retrieved_chunks, keywords):\n",
    "    relevances = [int(any(k in chunk.page_content for k in keywords)) for chunk in retrieved_chunks]\n",
    "    dcg = sum([(2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(relevances)])\n",
    "    ideal_relevances = sorted(relevances, reverse=True)\n",
    "    idcg = sum([(2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_relevances)])\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def calculate_keyword_coverage(retrieved_chunks, keywords):\n",
    "    found_keywords = set()\n",
    "    for chunk in retrieved_chunks:\n",
    "        for k in keywords:\n",
    "            if k in chunk.page_content:\n",
    "                found_keywords.add(k)\n",
    "    return len(found_keywords) / len(keywords) if keywords else 0.0\n",
    "\n",
    "def llm_judge(question, generated_answer, reference_answer):\n",
    "    gen_words = set(generated_answer.lower().split())\n",
    "    ref_words = set(reference_answer.lower().split())\n",
    "    common = gen_words.intersection(ref_words)\n",
    "    score = len(common) / max(len(ref_words), 1)\n",
    "    return {\n",
    "        \"score\": score,\n",
    "        \"common_words\": list(common),\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"reference_answer\": reference_answer\n",
    "    }\n",
    "\n",
    "def evaluate_retrieval(test_case, vector_store):\n",
    "    retrieved_chunks = vector_store.similarity_search(test_case['question'], k=5)\n",
    "    mrr = calculate_mrr(retrieved_chunks, test_case['keywords'])\n",
    "    ndcg = calculate_ndcg(retrieved_chunks, test_case['keywords'])\n",
    "    coverage = calculate_keyword_coverage(retrieved_chunks, test_case['keywords'])\n",
    "    return {\n",
    "        'mrr': mrr,\n",
    "        'ndcg': ndcg,\n",
    "        'keyword_coverage': coverage\n",
    "    }\n",
    "\n",
    "def evaluate_answer(test_case, rag_pipeline):\n",
    "    generated_answer = rag_pipeline.invoke({\"question\": test_case['question']})[\"answer\"]\n",
    "    evaluation = llm_judge(\n",
    "        question=test_case['question'],\n",
    "        generated_answer=generated_answer,\n",
    "        reference_answer=test_case['reference_answer']\n",
    "    )\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fea51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"Who won the prestigious award in 2023?\",\n",
    "        \"keywords\": [\"Maxine\", \"Thompson\", \"IoTY\"],\n",
    "        \"reference_answer\": \"Maxine Thompson won the prestigious Innovator of the Year award in 2023.\",\n",
    "        \"category\": \"direct_fact\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How long was Sarah Chen employed?\",\n",
    "        \"keywords\": [\"Sarah Chen\", \"2018\", \"2023\", \"five years\"],\n",
    "        \"reference_answer\": \"Sarah Chen was employed for five years, from 2018 to 2023.\",\n",
    "        \"category\": \"temporal\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run evaluation on first test case as example\n",
    "result = evaluate_answer(test_cases[0], conversation_chain)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d36c72",
   "metadata": {},
   "source": [
    "These modular functions enable independent testing of retrieval and generation components, helping you isolate where problems occur.\n",
    "\n",
    "### Running Systematic Experiments\n",
    "\n",
    "With your evaluation infrastructure in place, you can run controlled experiments to optimize your RAG pipeline. The process follows a scientific methodology:\n",
    "\n",
    "1. **Establish baseline**: Run complete evaluation on your current system configuration, recording all metrics\n",
    "2. **Form hypothesis**: Based on your understanding of RAG systems, hypothesize how a specific change might improve performance\n",
    "3. **Make controlled change**: Modify exactly one variable—chunk size, embedding model, number of retrieved chunks, etc.\n",
    "4. **Re-evaluate**: Run the same evaluation suite and compare results to baseline\n",
    "5. **Analyze results**: Determine whether the change improved, degraded, or had no effect on your metrics\n",
    "6. **Iterate**: Based on results, form new hypotheses and continue experimenting\n",
    "\n",
    "This systematic approach prevents the chaos of changing multiple variables simultaneously, which makes it impossible to understand what actually drives improvement.\n",
    "\n",
    "## Practical Experimentation: Chunking Strategies\n",
    "\n",
    "Let's examine how this evaluation framework guides real optimization decisions by exploring chunking strategy experiments.\n",
    "\n",
    "### Baseline Configuration\n",
    "\n",
    "Start with a reasonable default configuration:\n",
    "\n",
    "- Chunk size: 1000 characters\n",
    "- Chunk overlap: 200 characters\n",
    "- Text splitter: RecursiveCharacterTextSplitter\n",
    "- Retrieved chunks (K): 5\n",
    "- Embedding model: all-MiniLM-L6-v2\n",
    "\n",
    "Running evaluation on this configuration might yield:\n",
    "\n",
    "- MRR: 0.7298\n",
    "- NDCG: 0.7387\n",
    "- Keyword Coverage: 83.8%\n",
    "- Answer Accuracy: 3.99/5\n",
    "- Answer Completeness: 3.85/5\n",
    "- Answer Relevance: 4.57/5\n",
    "\n",
    "These baseline metrics provide your reference point for all subsequent experiments.\n",
    "\n",
    "### Experiment: Smaller Chunks\n",
    "\n",
    "**Hypothesis**: Smaller chunks will be more focused and semantically coherent, improving retrieval precision.\n",
    "\n",
    "**Changes**:\n",
    "\n",
    "- Chunk size: 1000 → 500 characters\n",
    "- Retrieved chunks: 5 → 10 (to maintain similar total context)\n",
    "\n",
    "**Rationale**: Halving chunk size doubles the number of chunks, so we double K to keep total context roughly constant. This isolates chunk size as the independent variable.\n",
    "\n",
    "**Results**:\n",
    "\n",
    "- MRR: 0.7604 (↑ improvement)\n",
    "- NDCG: 0.7821 (↑ improvement)\n",
    "- Keyword Coverage: 89.2% (↑ improvement)\n",
    "\n",
    "**Analysis**: Smaller chunks improved all retrieval metrics. The hypothesis appears confirmed—more granular chunks enabled better matching between questions and relevant content. The improvement likely stems from reducing the semantic diversity within each chunk, making vector representations more precise.\n",
    "\n",
    "### Experiment: Larger Chunks\n",
    "\n",
    "**Hypothesis**: Larger chunks preserve more context, potentially capturing complete concepts that span multiple sentences.\n",
    "\n",
    "**Changes**:\n",
    "\n",
    "- Chunk size: 1000 → 1667 characters\n",
    "- Retrieved chunks: 5 → 3 (to maintain similar total context)\n",
    "\n",
    "**Results**:\n",
    "\n",
    "- MRR: 0.7475 (↑ slight improvement over baseline, but worse than small chunks)\n",
    "- NDCG: 0.7456 (↔ minimal change)\n",
    "- Keyword Coverage: 85.1% (↑ slight improvement over baseline)\n",
    "\n",
    "**Analysis**: Larger chunks provided marginal improvement over the baseline but significantly underperformed smaller chunks. This suggests that for this particular knowledge base and question types, granularity outweighs the benefit of preserving broader context.\n",
    "\n",
    "### Experiment: Markdown-Aware Splitting\n",
    "\n",
    "**Hypothesis**: Using a text splitter designed specifically for Markdown documents will respect document structure, producing more semantically coherent chunks.\n",
    "\n",
    "**Changes**:\n",
    "\n",
    "- Text splitter: RecursiveCharacterTextSplitter → MarkdownTextSplitter\n",
    "- Chunk size: return to 500 characters\n",
    "- Retrieved chunks: 10\n",
    "\n",
    "**Results**:\n",
    "\n",
    "- MRR: 0.7380 (↓ regression from small chunks, slight improvement over original baseline)\n",
    "- NDCG: 0.7402 (↓ regression)\n",
    "- Keyword Coverage: 86.3% (↓ regression)\n",
    "\n",
    "**Analysis**: The Markdown splitter underperformed plain character splitting despite being theoretically better suited to the data format. Inspection reveals that Markdown section headers create chunks that are too large and unfocused. The lesson: domain-specific tools don't automatically outperform simpler approaches—empirical testing remains essential.\n",
    "\n",
    "## Beyond Basic Evaluation: Advanced Considerations\n",
    "\n",
    "As your RAG systems mature, evaluation frameworks can expand to address additional concerns:\n",
    "\n",
    "### Latency and Cost Metrics\n",
    "\n",
    "Production systems must balance quality with operational constraints. Track:\n",
    "\n",
    "- End-to-end query latency (retrieval + generation)\n",
    "- Per-query cost (embedding + retrieval + generation)\n",
    "- Throughput (queries per second)\n",
    "\n",
    "These operational metrics might reveal that an embedding model delivering 2% better accuracy costs 300% more and adds unacceptable latency. Real-world deployment often requires trading absolute quality for practical constraints.\n",
    "\n",
    "### Failure Analysis\n",
    "\n",
    "Beyond aggregate metrics, systematic failure analysis identifies patterns:\n",
    "\n",
    "- Which questions consistently fail across all configurations?\n",
    "- Are failures clustered by topic or category?\n",
    "- Do certain document types produce more retrieval failures?\n",
    "- Are failures correlated with question length or complexity?\n",
    "\n",
    "This analysis often reveals architectural limitations that parameter tuning cannot address, motivating research into advanced RAG techniques.\n",
    "\n",
    "### User Feedback Integration\n",
    "\n",
    "When possible, collect actual user feedback on answer quality. Users might rate answers on usefulness, correctness, or satisfaction. This real-world signal provides ground truth that synthetic evaluations approximate but cannot fully capture.\n",
    "\n",
    "However, user feedback comes with challenges: selection bias (users primarily provide feedback on poor answers), delay (feedback accumulates slowly), and interpretation difficulty (a low rating might reflect user dissatisfaction with the underlying reality rather than answer quality).\n",
    "\n",
    "### Regression Testing\n",
    "\n",
    "As you add test cases and modify your system, maintain regression testing discipline:\n",
    "\n",
    "- Run full evaluation suite before and after every significant change\n",
    "- Flag any performance degradation in previously passing test cases\n",
    "- Require explicit justification for accepting regressions in one area to improve another\n",
    "\n",
    "## Evaluation as Engineering Culture\n",
    "\n",
    "The most successful RAG projects treat evaluation not as a one-time validation step but as continuous discipline woven throughout development:\n",
    "\n",
    "**Evaluation-first development**: Before implementing new features or optimizations, define how you'll measure success. This prevents optimizing for vanity metrics or chasing improvements you cannot reliably detect.\n",
    "\n",
    "**Shared metrics across teams**: When multiple engineers work on RAG systems, standardized evaluation frameworks enable meaningful collaboration. Everyone optimizes toward the same goals using the same measurement approach.\n",
    "\n",
    "**Stakeholder communication**: Business stakeholders understand \"92% of questions retrieve relevant context\" or \"average answer accuracy of 4.2/5\" far better than abstract discussions of embedding dimensions or chunk overlap. Metrics provide a common language bridging technical and business perspectives.\n",
    "\n",
    "**Incremental improvement culture**: When evaluation becomes routine, teams naturally adopt incremental improvement mindsets. Each week brings small optimizations validated by metrics, compounding into substantial long-term gains.\n",
    "\n",
    "## Reflection and Next Steps\n",
    "\n",
    "This comprehensive exploration of evaluation frameworks transforms RAG development from art to science. You now possess the tools to:\n",
    "\n",
    "- Construct golden datasets that represent real user needs\n",
    "- Implement automated evaluation measuring both retrieval and answer quality\n",
    "- Run controlled experiments isolating individual variables\n",
    "- Analyze results quantitatively to guide optimization decisions\n",
    "- Recognize when aggregate metrics hide important category-specific patterns\n",
    "- Balance multiple competing metrics and operational constraints\n",
    "\n",
    "The evaluation principles presented here extend far beyond RAG to virtually any machine learning or AI system where output quality cannot be deterministically verified. The discipline of defining metrics, building test sets, and systematically measuring performance represents a transferable skill that will serve you throughout your career in AI engineering.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
