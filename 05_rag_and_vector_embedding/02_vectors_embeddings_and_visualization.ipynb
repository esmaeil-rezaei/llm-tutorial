{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74b9af4",
   "metadata": {},
   "source": [
    "# Vectors, Embeddings, and Vector Databases: A Deep Dive into RAG Architecture\n",
    "\n",
    "## Introduction to Vector-Based Retrieval\n",
    "\n",
    "The world of Retrieval-Augmented Generation represents one of the most transformative approaches in modern language model applications. At its core lies a deceptively simple but profoundly powerful concept: representing the semantic meaning of text as mathematical vectors in high-dimensional space. This transformation allows us to move beyond crude keyword matching toward genuinely understanding the intent and context behind queries.\n",
    "\n",
    "When building production-grade RAG systems, understanding the relationship between text, vectors, and retrieval becomes absolutely essential. This chapter explores these foundational concepts in depth, providing both theoretical understanding and practical implementation experience. By the end, you will have constructed your own vector database, visualized embeddings in multiple dimensions, and gained an intuitive grasp of how semantic similarity actually works at a mathematical level.\n",
    "\n",
    "## Introducing LangChain: Framework for LLM Applications\n",
    "\n",
    "LangChain emerged in October 2022 as an open-source framework created by Harrison Chase, who subsequently established a company around this increasingly popular tool. The framework serves as an abstraction layer that simplifies the process of working with multiple language models and chaining them together to accomplish complex tasks.\n",
    "\n",
    "### The Value Proposition of LangChain\n",
    "\n",
    "LangChain provides several compelling advantages for developers building LLM-powered applications. First and foremost, it dramatically accelerates development time for common patterns like RAG pipelines, agent systems, and summarization workflows. Tasks that might require dozens of lines of boilerplate code can often be accomplished in just a few lines using LangChain's high-level abstractions.\n",
    "\n",
    "The framework includes a robust ecosystem of tools and integrations. Whether you need to connect to different LLM providers, work with various vector databases, or implement complex agentic workflows, LangChain provides pre-built components that handle much of the complexity. This extensive tooling means you can rapidly prototype and iterate on ideas without constantly reinventing the wheel.\n",
    "\n",
    "From a career perspective, LangChain has achieved significant adoption in enterprise environments. Many production systems rely on LangChain infrastructure, making familiarity with the framework valuable for professional opportunities. Understanding LangChain demonstrates to potential employers that you can work with industry-standard tools for LLM application development.\n",
    "\n",
    "### The Trade-offs and Considerations\n",
    "\n",
    "However, no technology comes without drawbacks, and LangChain is no exception. When the framework first launched in 2022-2023, different LLM providers had markedly different APIs, making abstraction layers highly valuable. Today, the landscape has evolved considerably. Most major providers now offer OpenAI-compatible endpoints, making it remarkably straightforward to switch between models using simple configuration changes rather than framework abstractions.\n",
    "\n",
    "Additionally, many patterns that once required specialized frameworks have now standardized across the industry. Tool usage, prompt templates, and other common patterns now follow well-established conventions that don't necessarily require heavy abstraction layers. Lightweight alternatives like LiteLLM provide model switching capabilities with minimal learning curve.\n",
    "\n",
    "LangChain has also grown substantially over time, evolving from a lightweight abstraction into a more comprehensive and complex framework. It introduces its own terminology, concepts, and even its own query language called LCEL (LangChain Expression Language). This growth creates a steeper learning curve compared to more minimal alternatives. Some developers find that certain aspects of LangChain, such as its message handling conventions, feel somewhat dated compared to the simpler dictionary-based approaches that have become standard.\n",
    "\n",
    "The framework now exists as part of a larger ecosystem that includes LangGraph for agent workflows, LangSmith for observability, and other related products. While this ecosystem provides powerful capabilities, it also means committing to a substantial stack rather than picking and choosing lightweight components.\n",
    "\n",
    "### Making an Informed Decision\n",
    "\n",
    "Understanding both the strengths and limitations of LangChain allows you to make informed architectural decisions. For rapid prototyping and leveraging pre-built integrations, LangChain excels. For projects requiring minimal dependencies or maximum control over implementation details, lighter-weight alternatives might prove more appropriate. Throughout this course, you will gain hands-on experience with LangChain, allowing you to form your own opinions based on direct interaction with the framework.\n",
    "\n",
    "## Document Processing: The Art of Chunking\n",
    "\n",
    "Before we can perform vector-based retrieval, we must prepare our documents appropriately. This preparation process, called chunking, involves dividing documents into appropriately-sized segments. Understanding why and how we chunk documents is crucial for building effective RAG systems.\n",
    "\n",
    "### Why Chunking Matters\n",
    "\n",
    "Consider a comprehensive insurance policy document spanning dozens of pages. This document might contain information about coverage limits, claims procedures, exclusions, premium calculations, and customer service protocols. When a user asks a specific question like \"What is the deductible for water damage claims?\", the answer likely appears in just one or two paragraphs of this extensive document.\n",
    "\n",
    "If we created a single vector representing the entire document, that vector would somehow need to capture the semantic meaning of all the diverse topics contained within it. When we search for vectors similar to our specific question about water damage deductibles, a vector representing the entire document would likely have only weak similarity. The signal from the relevant paragraphs gets diluted by all the irrelevant content.\n",
    "\n",
    "Chunking solves this problem by breaking documents into focused segments. Each chunk receives its own vector embedding, allowing specific portions of documents to match strongly with relevant queries. This granular approach dramatically improves retrieval accuracy.\n",
    "\n",
    "### Chunking Strategies and Trade-offs\n",
    "\n",
    "Determining the optimal chunk size involves balancing several competing concerns. Chunks that are too small might lack sufficient context to be meaningful or might split important information across multiple segments. Chunks that are too large face the same dilution problems as embedding entire documents.\n",
    "\n",
    "The field of RAG engineering treats chunking as an experimental, empirical process rather than a solved problem with universal rules. Different types of content, different query patterns, and different use cases may benefit from different chunking strategies. You will often need to experiment with various approaches and evaluate which performs best for your specific application.\n",
    "\n",
    "LangChain provides several text splitter classes to facilitate experimentation. The most basic is the `CharacterTextSplitter`, which simply divides text based on character count. A more sophisticated variant, the `RecursiveCharacterTextSplitter`, employs a hierarchical approach. It first attempts to split documents at natural boundaries like double line breaks between sections, then falls back to single line breaks, then sentence endings, and finally individual characters if necessary.\n",
    "\n",
    "### Implementing Chunk Overlap\n",
    "\n",
    "An important refinement in chunking strategy involves creating overlap between consecutive chunks. Consider a scenario where the answer to a user's question spans across what would naturally become the boundary between two chunks. Without overlap, we might accidentally split the answer, degrading our retrieval quality.\n",
    "\n",
    "By introducing overlap—typically 10-20% of the chunk size—we create redundancy that helps ensure important information isn't inadvertently divided. A 200-character overlap on 1000-character chunks means each chunk shares its final 200 characters with the beginning of the next chunk, creating a buffer zone that protects against unfortunate boundary placement.\n",
    "\n",
    "### Practical Implementation with LangChain\n",
    "\n",
    "Let's examine the practical code for implementing document chunking. First, we need to import the necessary LangChain components. LangChain's modular architecture means importing from multiple specialized packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d26d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554123f",
   "metadata": {},
   "source": [
    "The `DirectoryLoader` class provides convenient functionality for loading all documents from a specified directory structure. The `RecursiveCharacterTextSplitter` implements the hierarchical splitting strategy we discussed.\n",
    "\n",
    "To load documents from our knowledge base directory structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4649284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "\n",
    "\n",
    "documents = []\n",
    "folders = ['company', 'contracts', 'employees', 'products']\n",
    "\n",
    "for folder in folders:\n",
    "    doc_type = folder\n",
    "    loader = DirectoryLoader(\n",
    "        f'knowledge_base/{folder}',\n",
    "        glob='**/*.md',\n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs=text_loader_kwargs\n",
    "    )\n",
    "    folder_docs = loader.load()\n",
    "    \n",
    "    for doc in folder_docs:\n",
    "        doc.metadata['doc_type'] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b123e484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78c6360d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'knowledge_base/company/02_mission_and_values.md', 'doc_type': 'company'}, page_content='# InsureAll – Mission and Values\\n\\n## Mission\\n\\nInsureAll’s mission is to protect people and organizations from financial uncertainty by providing dependable and understandable insurance solutions.\\n\\n## Core Values\\n\\n- **Trust**  \\n  Building long-term relationships through honesty and consistency.\\n\\n- **Transparency**  \\n  Clear communication of policy terms, pricing, and coverage details.\\n\\n- **Reliability**  \\n  Delivering dependable service across underwriting, claims, and support.\\n\\n- **Innovation**  \\n  Using technology to improve customer experience and operational efficiency.\\n\\nThese values guide decision-making across all departments within InsureAll.\\n')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8507de5f",
   "metadata": {},
   "source": [
    "This code walks through each folder in our knowledge base, loads all Markdown files, and attaches metadata indicating the document type. Each document becomes a LangChain `Document` object containing both content and metadata.\n",
    "\n",
    "Now we can split these documents into chunks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81f746b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 16 chunks from 12 documents\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716407a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'knowledge_base/company/01_company_overview.md', 'doc_type': 'company'}, page_content='Key values guiding InsureAll include trust, reliability, fairness, and innovation. These values shape product development, customer service, and long-term partnerships with policyholders and stakeholders.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f55cd3",
   "metadata": {},
   "source": [
    "This produces our collection of document fragments, each approximately 1000 characters long with 200 characters of overlap. These chunks become the atomic units that we'll embed and store in our vector database.\n",
    "\n",
    "## Understanding Encoders and Embedding Models\n",
    "\n",
    "The transformation of text into vectors represents one of the most crucial steps in RAG systems. This transformation is performed by specialized models called encoders or embedding models. Understanding how these models work and how to choose appropriate ones for your use case is essential for building high-quality retrieval systems.\n",
    "\n",
    "### The Evolution of Embedding Models\n",
    "\n",
    "The journey of embedding models began with Word2Vec, an early approach that could represent individual words as vectors. This pioneering work demonstrated that vector arithmetic could capture semantic relationships—the famous example being that \"king\" minus \"man\" plus \"woman\" approximately equals \"queen\" in vector space.\n",
    "\n",
    "BERT arrived in 2018 as Google's transformer-based encoder, representing a massive leap forward in understanding context. BERT embeddings could capture how the same word might have different meanings in different contexts, something static word embeddings couldn't achieve.\n",
    "\n",
    "Modern embedding models have continued this evolution. Today's encoders are specifically trained to produce embeddings that excel at semantic similarity tasks, making them ideal for retrieval applications.\n",
    "\n",
    "### Popular Embedding Models\n",
    "\n",
    "Several embedding models have emerged as industry standards, each with distinct characteristics:\n",
    "\n",
    "OpenAI offers the `text-embedding-3-small` and `text-embedding-3-large` models. The small variant produces 1536-dimensional vectors and provides excellent performance at low cost. The large variant generates 3072-dimensional vectors with even stronger semantic understanding, suitable for demanding applications where maximum accuracy justifies higher costs.\n",
    "\n",
    "Google provides `text-embedding-004` (previously named `embedding-001`), which has gained popularity in the Gemini ecosystem and offers strong multilingual capabilities.\n",
    "\n",
    "From the open-source world, Hugging Face hosts numerous embedding models through their Sentence Transformers library. The most widely used is `all-MiniLM-L6-v2`, which produces 384-dimensional embeddings. This model has become ubiquitous in the community due to its good performance, compact size, and zero cost for self-hosting.\n",
    "\n",
    "### Embedding Dimensions and Model Capability\n",
    "\n",
    "A common misconception is that models producing higher-dimensional embeddings are automatically superior. While there is often correlation between dimensionality and capability, the relationship is not purely causal. Higher dimensions provide more degrees of freedom for the model to express semantic nuances, but what truly matters is the quality of the model's training and architecture.\n",
    "\n",
    "A well-trained model producing 384-dimensional vectors might outperform a poorly-trained model generating 1024-dimensional vectors. The dimensionality tells us about the representation space, but the model's ability to capture meaning determines actual performance.\n",
    "\n",
    "### The Critical Distinction: Encoders vs. Vector Databases\n",
    "\n",
    "A source of frequent confusion deserves explicit clarification: embedding models and vector databases serve entirely different purposes in the RAG pipeline. The embedding model creates vectors by processing text through a neural network trained to capture semantic meaning. The vector database stores these pre-computed vectors and enables fast similarity searches.\n",
    "\n",
    "When you create a vector database using code like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b0a7f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using free HuggingFaceEmbeddings\n",
    "# vector_store = Chroma.from_documents(\n",
    "#     documents=chunks,\n",
    "#     embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"),\n",
    "#     persist_directory=\"vector_db\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d37ddb",
   "metadata": {},
   "source": [
    "You are simultaneously specifying both the encoder (HuggingFaceEmbeddings) and the storage system (Chroma). The syntax might make it appear that these are tightly coupled, but they remain conceptually and functionally separate. You could swap the embedding model while keeping the same vector database, or vice versa.\n",
    "\n",
    "## Vector Databases: Storage and Retrieval Infrastructure\n",
    "\n",
    "With a clear understanding of how vectors are created, we can now examine where and how they are stored. Vector databases are specialized data stores optimized for storing high-dimensional vectors and performing rapid similarity searches.\n",
    "\n",
    "### The Vector Database Landscape\n",
    "\n",
    "The vector database ecosystem has expanded rapidly, offering numerous options across different deployment models and price points.\n",
    "\n",
    "Open-source solutions include Chroma, which we'll use extensively in this course. Chroma is remarkably easy to set up—it requires no separate database server and stores everything in local files using SQLite as its backing store. This simplicity makes it perfect for development and prototyping.\n",
    "\n",
    "FAISS (Facebook AI Similarity Search), developed by Meta, represents another popular open-source option. However, FAISS differs from traditional databases in that it provides an in-memory similarity search library rather than persistent storage. It excels at pure similarity computations but requires additional infrastructure for durability and data management.\n",
    "\n",
    "Commercial offerings like Pinecone and Weaviate provide managed vector database services. These platforms handle scaling, replication, backups, and other operational concerns, making them attractive for production deployments where you want to focus on application logic rather than database administration.\n",
    "\n",
    "### Traditional Databases Enter the Vector Space\n",
    "\n",
    "An important recent trend involves traditional database systems adding native vector support. PostgreSQL with the pgvector extension, MongoDB, and Elasticsearch all now support vector storage and similarity search alongside their core functionality.\n",
    "\n",
    "This convergence is significant because it reduces the need for separate specialized vector databases in many scenarios. If you're already using Postgres for your application data, adding vector embeddings to the same database simplifies your architecture considerably. You can perform hybrid queries that combine traditional filtering with vector similarity, all within a single system.\n",
    "\n",
    "Elasticsearch, in particular, excels at handling massive vector datasets. Production systems routinely store hundreds of millions of vectors in Elasticsearch clusters, performing similarity searches that filter across both vectors and structured attributes in mere seconds.\n",
    "\n",
    "### Choosing a Vector Database\n",
    "\n",
    "The choice of vector database primarily represents an infrastructure decision rather than an algorithmic one. Factors to consider include:\n",
    "\n",
    "- **Cost**: Open-source options like Chroma and FAISS are free but require you to manage them. Commercial options charge based on usage but handle operations.\n",
    "\n",
    "- **Scale**: How many vectors will you store? Millions? Billions? Different databases excel at different scales.\n",
    "\n",
    "- **Performance**: What query latency requirements do you have? Some databases are optimized for throughput, others for low latency.\n",
    "\n",
    "- **Integration**: Does the database integrate well with your existing infrastructure? Can it perform hybrid queries combining vectors with other data?\n",
    "\n",
    "- **Operational complexity**: Who will manage the database? Do you have the expertise and resources for self-hosting?\n",
    "\n",
    "In contrast, the choice of embedding model dramatically affects the quality of your retrieval results and requires careful evaluation and testing. Prioritize experimenting with different encoders over agonizing about vector database selection.\n",
    "\n",
    "## Implementing Vector Storage with Chroma\n",
    "\n",
    "With theoretical foundations established, let's implement a complete vector storage system using Chroma and the all-MiniLM-L6-v2 embedding model.\n",
    "\n",
    "### Setting Up the Embedding Model\n",
    "\n",
    "First, we instantiate our embedding model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7507f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# embeddings = OpenAIEmbeddings(\n",
    "#     model=\"text-embedding-3-small\"\n",
    "# )\n",
    "# embeddings = OpenAIEmbeddings(\n",
    "#     model=\"text-embedding-3-large\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716515a8",
   "metadata": {},
   "source": [
    "This creates an embedding model instance that will handle converting our text chunks into 384-dimensional vectors. The model downloads automatically on first use and caches locally for subsequent runs.\n",
    "\n",
    "### Creating the Vector Database\n",
    "\n",
    "Now we can create our Chroma vector database in a single operation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea82043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing database\n",
    "if os.path.exists(\"vector_db\"):\n",
    "    shutil.rmtree(\"vector_db\")\n",
    "\n",
    "# Create new vector store\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"vector_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82edf7",
   "metadata": {},
   "source": [
    "This code performs several operations automatically. For each chunk in our chunks list, it calls the embedding model to generate a vector. It then stores both the vector and the original text (along with metadata) in the Chroma database. Finally, it persists everything to disk in the specified directory.\n",
    "\n",
    "> Note that FAISS is another method for vectorization.\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "vector_store = FAISS.from_documents(...)\n",
    "```\n",
    "\n",
    "### Examining the Vector Database\n",
    "\n",
    "We can inspect what we've created:\n",
    "\n",
    "```\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a29aff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors: 16\n",
      "Vector dimensions: 1536\n"
     ]
    }
   ],
   "source": [
    "collection = vector_store._collection\n",
    "print(f\"Number of vectors: {collection.count()}\")\n",
    "\n",
    "# Get a sample vector to check dimensionality\n",
    "sample_embedding = collection.get(limit=1, include=['embeddings'])\n",
    "vector_dim = len(sample_embedding['embeddings'][0])\n",
    "print(f\"Vector dimensions: {vector_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a105db",
   "metadata": {},
   "source": [
    "This reveals that we've stored 413 vectors (one for each chunk), and each vector has 384 dimensions, confirming our embedding model is working as expected.\n",
    "\n",
    "## Visualizing Vector Embeddings\n",
    "\n",
    "Perhaps the most illuminating aspect of working with embeddings is visualizing them. While vectors exist in high-dimensional space that we cannot directly perceive, dimensionality reduction techniques allow us to project them into 2D or 3D space for visualization.\n",
    "\n",
    "### The Curse and Gift of Dimensionality\n",
    "\n",
    "Human perception is limited to three spatial dimensions. We simply cannot visualize a point in 384-dimensional space, let alone understand the relationships between thousands of such points. This presents a challenge when trying to develop intuition about embedding spaces.\n",
    "\n",
    "Fortunately, mathematical techniques exist for dimensionality reduction. These algorithms take high-dimensional data and project it into lower dimensions while attempting to preserve important structural properties—particularly the relative distances between points.\n",
    "\n",
    "### t-SNE: Visualizing High-Dimensional Data\n",
    "\n",
    "The t-SNE (t-Distributed Stochastic Neighbor Embedding) algorithm has become a standard tool for visualizing embeddings. t-SNE works by constructing probability distributions over pairs of points in both the high-dimensional and low-dimensional spaces, then optimizing the low-dimensional representation to match the high-dimensional distribution as closely as possible.\n",
    "\n",
    "The key property that t-SNE preserves is this: if two points are close together in high-dimensional space, t-SNE tries to keep them close in the low-dimensional projection. Similarly, distant points should remain distant. This means clustering and separation patterns visible in 2D or 3D generally reflect genuine structure in the original high-dimensional space.\n",
    "\n",
    "### Creating 2D Visualizations\n",
    "\n",
    "Let's implement a 2D visualization of our embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "003c4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.io as pio\n",
    "\n",
    "# Use browser renderer to avoid notebook MIME errors\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "# Extract vectors and metadata from our database\n",
    "results = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = results['embeddings']\n",
    "documents = results['documents']\n",
    "metadatas = results['metadatas']\n",
    "\n",
    "vectors = np.array(vectors, dtype=np.float32)\n",
    "doc_types = [meta['doc_type'] for meta in metadatas]\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    random_state=42,\n",
    "    perplexity=8\n",
    ")\n",
    "vectors_2d = tsne.fit_transform(vectors)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'x': vectors_2d[:, 0],\n",
    "    'y': vectors_2d[:, 1],\n",
    "    'doc_type': doc_types,\n",
    "    'text': documents\n",
    "})\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='x',\n",
    "    y='y',\n",
    "    color='doc_type',\n",
    "    hover_data=['text'],\n",
    "    title='2D Projection of Document Embeddings'\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bfa5d1",
   "metadata": {},
   "source": [
    "This produces an interactive scatter plot where each point represents one chunk. Hovering over points reveals the actual text content. The coloring by document type helps us assess whether semantically similar documents cluster together.\n",
    "\n",
    "### Interpreting the Visualization\n",
    "\n",
    "When examining the 2D plot, several patterns typically emerge. Documents of the same type often cluster together, even though the embedding model never received explicit information about document types. The model learned to place employee-related chunks near each other purely based on semantic similarity of their content.\n",
    "\n",
    "However, you'll also notice interesting overlaps. Product descriptions might appear near contract sections that discuss products. Employee records might cluster near company information that discusses employee benefits. These overlaps reflect genuine semantic similarities—the content truly is related across these document types.\n",
    "\n",
    "The axes themselves have no inherent meaning. t-SNE optimization can rotate, flip, or scale the visualization arbitrarily. What matters is the relative positioning of points and the clustering patterns that emerge.\n",
    "\n",
    "### Extending to 3D Visualization\n",
    "\n",
    "Human perception can go one dimension further. Let's create a 3D visualization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07fb97f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_3d = TSNE(\n",
    "    n_components=3,\n",
    "    random_state=42,\n",
    "    perplexity=8\n",
    ")\n",
    "vectors_3d = tsne_3d.fit_transform(vectors)\n",
    "\n",
    "df_3d = pd.DataFrame({\n",
    "    'x': vectors_3d[:, 0],\n",
    "    'y': vectors_3d[:, 1],\n",
    "    'z': vectors_3d[:, 2],\n",
    "    'doc_type': doc_types,\n",
    "    'text': documents\n",
    "})\n",
    "\n",
    "fig_3d = px.scatter_3d(\n",
    "    df_3d,\n",
    "    x='x',\n",
    "    y='y',\n",
    "    z='z',\n",
    "    color='doc_type',\n",
    "    hover_data=['text'],\n",
    "    title='3D Projection of Document Embeddings'\n",
    ")\n",
    "fig_3d.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
