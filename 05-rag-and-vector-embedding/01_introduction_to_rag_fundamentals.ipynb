{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4300ca7",
   "metadata": {},
   "source": [
    "# Understanding Retrieval Augmented Generation: From Concept to Implementation\n",
    "\n",
    "## Introduction to RAG and Course Progress\n",
    "\n",
    "Welcome to one of the most transformative topics in modern large language model engineering. At this point in your learning journey, you've already built a solid foundation that positions you perfectly to tackle advanced concepts. You understand how to work with frontier models through their APIs, create sophisticated AI assistants equipped with various tools, leverage open-source models for diverse applications, and make informed decisions about model selection based on concrete performance metrics and benchmarks. You've even developed solutions that generate code using both cutting-edge commercial models and their open-source counterparts.\n",
    "\n",
    "Today marks a pivotal shift in your understanding. Rather than diving straight into implementation details, we're going to explore the fundamental concepts that make Retrieval Augmented Generation such a powerful technique. This conceptual foundation will serve you well, because once you truly grasp the underlying principles, you'll realize that RAG, despite its sophisticated-sounding name, rests on remarkably intuitive ideas.\n",
    "\n",
    "## The Foundational Concepts You've Already Mastered\n",
    "\n",
    "Before we venture into new territory, let's acknowledge the impressive toolkit you've already assembled. Working with frontier models has taught you how the Chat Completions API operates and how to structure effective prompts. You've built user interfaces that demonstrate your understanding of multi-modal capabilities, processing not just text but images and other data types. Your exploration of open-source models through platforms like Hugging Face has given you practical experience with model selection and deployment. Perhaps most importantly, you've developed the critical thinking skills necessary to evaluate models objectively, using benchmarks and metrics rather than marketing claims or assumptions.\n",
    "\n",
    "All of these skills converge beautifully in the context of RAG. This technique doesn't require you to abandon what you've learned; instead, it builds upon your existing knowledge in elegant and practical ways.\n",
    "\n",
    "## What RAG Actually Means\n",
    "\n",
    "The term \"Retrieval Augmented Generation\" might sound imposing at first, but let's demystify it immediately. When you break down this concept to its core, you'll find it's built on ideas so intuitive that you've likely considered them yourself, even if you didn't know they had a formal name.\n",
    "\n",
    "### The Small Idea: Enhanced Context Through Information Retrieval\n",
    "\n",
    "Think back to all the prompting techniques you've already used. When you provide few-shot examples in your prompts, you're essentially giving the model additional context to work with. When you use tools like function calling, you're allowing the model to access specific information it needs to answer questions accurately. Even the simple act of crafting detailed prompts is fundamentally about providing context.\n",
    "\n",
    "The small idea behind RAG is simply this: what if we could systematically and automatically provide relevant context for every user query? Instead of manually deciding what information to include in each prompt, what if we built a system that intelligently retrieves and includes relevant information based on what the user is asking?\n",
    "\n",
    "Consider a practical scenario. Imagine you're building an AI assistant for a travel company. This assistant needs to answer questions about flight prices, destination information, accommodation options, and local activities. You could try to include all this information in every single prompt, but that would be wasteful and would quickly exceed context window limits. Alternatively, you could manually decide what information to include based on each specific question, but that's time-consuming and error-prone.\n",
    "\n",
    "The elegant solution is to create a knowledge base—essentially a structured collection of all the information your assistant might need. When a user asks a question, your system first examines that question and retrieves relevant information from the knowledge base. This retrieved information is then automatically included in the prompt sent to the language model, along with the user's original question.\n",
    "\n",
    "Here's a concrete example of how this works. Suppose your knowledge base contains detailed information about flights to various cities. A user asks: \"What are the ticket prices to London?\" Your system would detect that this question involves London, retrieve all relevant information about London flights from the knowledge base, and construct a prompt that includes both the question and the retrieved information. The language model then generates its response based on this enriched context, providing accurate, specific answers rather than generic or hallucinated information.\n",
    "\n",
    "This approach works because language models are fundamentally pattern-matching systems. When you provide relevant information in the prompt, the model is naturally biased toward generating responses that are coherent with that context. If the prompt contains specific flight prices, the model is highly likely to reference those exact prices in its response rather than making up numbers.\n",
    "\n",
    "### A Practical Implementation: The Naive Approach\n",
    "\n",
    "Let's make this concrete with a simplified example that, while basic, illustrates the core principle. Imagine you're working for an insurance technology startup called InsureAll. This company has a shared drive full of documents: employee records, product descriptions, contract details, and company policies. Your task is to build an AI assistant that can answer expert-level questions about the company.\n",
    "\n",
    "The most straightforward implementation would work like this: First, you read all the documents from the company's shared drive and store them in memory. For organizational purposes, you might index these documents by key terms—perhaps employee last names or product names. When a user asks a question, you scan through the words in their question looking for any that match your indexed terms. If you find a match, you retrieve the entire associated document and include it in your prompt to the language model.\n",
    "\n",
    "For instance, if someone asks \"Who is Alex Ferreira?\" and you have an employee record indexed under \"Alex Ferreira,\" you would retrieve that entire record and include it in your prompt: \"You are an expert assistant for InsureAll. Here is potentially relevant context: [employee record]. Now answer this question: Who is Alex Ferreira?\"\n",
    "\n",
    "This approach, while crude, actually works reasonably well for simple cases. It demonstrates the fundamental RAG pattern: question comes in, relevant information is retrieved, augmented prompt is sent to the model, enhanced response is generated.\n",
    "\n",
    "However, this naive implementation has obvious limitations. What if the user asks about someone using their first name instead of their last name? What if there's a typo in the question? What if the question uses synonyms or related terms that don't exactly match your index? The simple string-matching approach breaks down quickly in real-world scenarios.\n",
    "\n",
    "## Building the Simple RAG System\n",
    "\n",
    "Let's walk through implementing this simplified version to build intuition. The implementation uses straightforward Python code that even beginners can understand and modify.\n",
    "\n",
    "First, we establish our setup: importing necessary libraries, configuring our OpenAI API key, and selecting an efficient model for our task. For this demonstration, we'll use GPT-4 Mini (or GPT-4 Nano), which offers an excellent balance of capability and cost-effectiveness for most RAG applications.\n",
    "\n",
    "Next, we need to load our knowledge base. In this example, our knowledge base consists of markdown files organized in directories: one for company information, one for contracts, one for employee records, and one for product descriptions. Each file contains detailed information about its subject. These files were generated synthetically using a language model—a technique you likely explored in earlier coursework—which means they contain realistic but fictional data.\n",
    "\n",
    "The code to load this data is straightforward. We use Python's file system operations to walk through each directory, reading the contents of each markdown file. For simplicity, we extract a key identifier from each filename (such as an employee's last name or a product name) and use that as the key in a Python dictionary. The value associated with each key is the full text content of that file.\n",
    "\n",
    "Here's where it gets interesting. When a user submits a question, we need to determine which documents from our knowledge base might be relevant. Our naive approach does this by parsing the user's question into individual words, removing any punctuation, and converting everything to lowercase. We then iterate through each word and check if it matches any key in our dictionary. If we find matches, we retrieve all the associated documents.\n",
    "\n",
    "This retrieval process happens before we ever call the language model. We're essentially preprocessing the user's question to gather relevant context. Once we have this context, we construct a system prompt that includes two components: first, general instructions about the assistant's role and behavior, and second, the retrieved contextual information prefaced with something like \"The following additional context might be relevant to the user's question.\"\n",
    "\n",
    "The complete prompt sent to the language model therefore consists of the system instructions, any retrieved context, the conversation history (if this is part of an ongoing dialogue), and the user's current question. The language model processes all of this and generates a response that naturally incorporates the provided context.\n",
    "\n",
    "### Observing the System in Action\n",
    "\n",
    "When you run this system, you can see the RAG pattern in action. Ask \"Who is Alex Ferreira?\" and the system retrieves the employee record for the person with that last name, includes it in the prompt, and the language model responds with accurate information drawn from that record. Ask \"What is CarLLM?\" (one of the product names) and the system retrieves the product documentation and provides a detailed answer.\n",
    "\n",
    "The system works remarkably well for straightforward queries where the question contains exact matches to our indexed terms. However, the limitations become apparent quickly. Ask \"Who is Avery?\" (using just the first name) and the system fails to retrieve the relevant employee record because we only indexed by last name. Ask about \"Alex Alex Ferreira\" when no such person exists, and the system mistakenly retrieves information about \"Alex Ferreira\" because it found that term in the question, even though it's attached to the wrong first name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85da0e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: Alex Ferreira is a Data Scientist\n"
     ]
    }
   ],
   "source": [
    "# Our database (The \"Knowledge\")\n",
    "database = [\"Alex Ferreira is a Data Scientist\", \"CarLLM is our AI product\"]\n",
    "\n",
    "# 1. SUCCESS: Exact match works\n",
    "query_1 = \"Alex Ferreira\"\n",
    "if query_1 in database[0]:\n",
    "    print(\"Found: \" + database[0])\n",
    "else:\n",
    "    print(\"Not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97e265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for A. Ferreira: Not found (String match failed)\n"
     ]
    }
   ],
   "source": [
    "# 2. FAILURE: \"A. Ferreira\" is not the string \"Alex Ferreira\", even if the person is the same\n",
    "query_2 = \"A. Ferreira Data Scientist\"\n",
    "if query_2 in database[0]:\n",
    "    print(\"Found: \" + database[0])\n",
    "else:\n",
    "    print(\"Result for A. Ferreira: Not found (String match failed)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d190248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for alex: Not found (Exact matching is case-sensitive)\n"
     ]
    }
   ],
   "source": [
    "# 3. FAILURE: Case sensitivity\n",
    "query_3 = \"alex ferreira\" # Lowercase 'a'\n",
    "if query_3 in database[0]:\n",
    "    print(\"Found\")\n",
    "else:\n",
    "    print(\"Result for alex: Not found (Exact matching is case-sensitive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63722c05",
   "metadata": {},
   "source": [
    "These failures aren't bugs in our code—they're fundamental limitations of the string-matching approach. They illustrate why we need something more sophisticated, something that understands meaning rather than just matching text strings.\n",
    "\n",
    "There's another subtle behavior worth noting. If you're having an ongoing conversation with the system, the conversation history itself becomes part of the context. This means the model might correctly answer a question even if the current turn doesn't retrieve the necessary information, because that information was retrieved and discussed in earlier turns of the conversation. This can actually mask problems with your retrieval system during testing, so it's important to start fresh conversations when you want to test whether specific queries successfully retrieve the right context.\n",
    "\n",
    "## The Big Idea: Semantic Search Through Vector Embeddings\n",
    "\n",
    "Now we come to the transformative insight that makes modern RAG systems so powerful. The limitation of our simple string-matching approach is that it only finds exact textual matches. What we really want is semantic matching—finding information based on meaning rather than exact wording.\n",
    "\n",
    "This is where vector embeddings enter the picture. To understand embeddings, we first need to distinguish between two fundamentally different types of language models, both of which play crucial roles in modern AI systems.\n",
    "\n",
    "### Two Types of Language Models\n",
    "\n",
    "The language models you've been working with so far—GPT-4, Claude, Gemini, and others—are all what we call **autoregressive language models**. These models are trained to perform one specific task: given a sequence of input tokens, predict the next token that should follow. They do this one token at a time, taking the original input plus all previously generated tokens to predict each successive token. This autoregressive process is what enables them to generate coherent, extended responses.\n",
    "\n",
    "However, there's another category of language models that work quite differently. These are called **encoder models, embedding models, or autoencoder models**. Instead of generating text one token at a time, these models take a complete input sequence and produce a single output that represents the entire input. This output isn't text—it's a mathematical representation of the input's meaning.\n",
    "\n",
    "### Understanding Vector Embeddings\n",
    "\n",
    "When an encoder model processes text, it produces what we call a vector embedding: a list of numbers that represents the meaning of that text. You can think of this as coordinates in a high-dimensional space. If the embedding has three numbers, you could visualize it as a point in three-dimensional space with X, Y, and Z coordinates. If it has 1,000 numbers (which is common for modern embedding models), it represents a point in 1,000-dimensional space—a concept that's impossible to visualize but mathematically well-defined.\n",
    "\n",
    "The crucial property of these embeddings is this: texts with similar meanings produce vectors that are close together in this high-dimensional space, even if the actual words used are completely different. Consider these two sentences:\n",
    "\n",
    "- \"What are ticket prices to travel from New York to London?\"\n",
    "- \"What's the cost of a flight from JFK to Heathrow Airport?\"\n",
    "\n",
    "These sentences use entirely different words, but they mean essentially the same thing. When converted to vector embeddings by a good encoder model, they would produce points that are very close together in embedding space.\n",
    "\n",
    "This property is what makes embeddings so powerful for search. Instead of looking for exact text matches, we can search for semantic similarity—finding content that means the same thing as our query, regardless of the specific words used.\n",
    "\n",
    "### The Mathematics of Meaning\n",
    "\n",
    "There's something even more fascinating about vector embeddings. Researchers discovered that you can perform meaningful mathematical operations on these vectors, and the results reflect operations on meaning itself.\n",
    "\n",
    "This discovery was first made with a technique called **Word2Vec**, which predated modern transformer models. With Word2Vec, individual words were converted to vectors, and researchers found something remarkable: you could add and subtract these vectors in meaningful ways.\n",
    "\n",
    "The classic example is: King - Man + Woman = Queen. Take the vector for \"King,\" subtract the vector for \"Man,\" add the vector for \"Woman,\" and the resulting point in vector space is closest to the vector for \"Queen.\" Similarly, Paris - France + England = London.\n",
    "\n",
    "This isn't just a mathematical curiosity—it reveals something profound about how these models represent meaning. The vector representation captures not just the identity of a word but its relationships to other concepts. The difference between \"King\" and \"Man\" represents a particular dimension of meaning (roughly, royalty or rulership), and this same dimension can be added to \"Woman\" to arrive at \"Queen.\"\n",
    "\n",
    "Modern encoder models extend this capability to entire documents. You can take a paragraph of text, convert it to a vector, and perform semantic operations: removing concepts, adding concepts, and moving through semantic space in meaningful ways.\n",
    "\n",
    "### Tokens Versus Vectors: Clearing Up a Common Confusion\n",
    "\n",
    "A question that often arises at this point: what's the difference between tokens and vectors? They're fundamentally different concepts serving different purposes, and it's worth clarifying this distinction.\n",
    "\n",
    "Tokens are simply a numerical representation of text input. Since neural networks can only process numbers, not words, we need a way to convert text into numbers. We could assign each word a unique number, but that has problems—what about new words? What about spelling variations? The solution is subword tokenization, where we break text into smaller units (neither whole words nor individual characters, but something in between) and assign numbers to those units. These numbers are the tokens.\n",
    "\n",
    "Tokenization is a preprocessing step that happens before any model sees the text. It's a deterministic, rule-based process. You can think of tokens as just another format for representing text, like ASCII codes represent characters.\n",
    "\n",
    "Vectors, in contrast, are the output of a neural network that has processed and understood the text. An encoder model takes tokens as input (just like any language model), processes them through many layers of neural computation, and produces vectors that represent the semantic content of that text. These vectors capture meaning, relationships, and context in ways that simple tokens cannot.\n",
    "\n",
    "Here's an analogy: tokens are like translating a book into another language—you've changed the format but haven't added any understanding. Vectors are like having someone read and comprehend that book, then provide a summary that captures its essence. The vector is an understanding of the content, not just a reformatting of it.\n",
    "\n",
    "## The Complete RAG Architecture\n",
    "\n",
    "Now we can see how all these pieces fit together to create a sophisticated RAG system. Let's trace the flow of information through a complete RAG pipeline.\n",
    "\n",
    "### Indexing Phase: Preparing Your Knowledge Base\n",
    "\n",
    "Before your system can answer any questions, you need to prepare your knowledge base. This happens once, during a setup or indexing phase, and doesn't need to be repeated unless your knowledge base changes.\n",
    "\n",
    "First, you collect all the documents, data, and information that you want your system to be able to draw upon. This might be product documentation, employee records, company policies, research papers, or any other relevant content.\n",
    "\n",
    "Next, you select an appropriate encoder model. Popular choices include OpenAI's text-embedding models (like text-embedding-3-large), open-source models like BGE or all-MiniLM, or specialized domain-specific embedding models. The choice depends on your specific needs: the size of your documents, the nature of your domain, performance requirements, and cost considerations.\n",
    "\n",
    "You then process each document through this encoder model, generating a vector embedding for each piece of content. These embeddings, along with the original text they represent, are stored in what's called a vector database or vector store. Popular vector stores include Chroma, Pinecone, Weaviate, and Qdrant, among others. These databases are specifically optimized for storing vectors and efficiently finding similar vectors—a task that would be impractical with traditional databases.\n",
    "\n",
    "### Query Phase: Answering User Questions\n",
    "\n",
    "When a user asks a question, the system follows this process:\n",
    "\n",
    "First, the user's question is converted into a vector using the same encoder model you used to embed your knowledge base. This is crucial—you must use the same model for both indexing and querying, because different models produce vectors in different semantic spaces that aren't directly comparable.\n",
    "\n",
    "With the question represented as a vector, you query your vector database: \"Find me the documents whose embeddings are closest to this question's embedding.\" The vector database uses efficient algorithms (often approximate nearest neighbor search) to quickly identify the most semantically similar documents, even if your knowledge base contains millions of entries.\n",
    "\n",
    "These retrieved documents—the actual text, not the vectors—are then incorporated into a prompt sent to your generation model (the autoregressive language model). The prompt includes your system instructions, the retrieved context (often prefaced with something like \"Here is relevant background information\"), and the user's question.\n",
    "\n",
    "The language model generates a response based on all this information. Because the prompt contains specific, relevant information retrieved based on semantic similarity, the response is both accurate and grounded in your actual knowledge base rather than the model's training data.\n",
    "\n",
    "Finally, the response is returned to the user. To them, it appears as if the AI has deep, specific knowledge about your domain, when in reality you've engineered a system that dynamically retrieves and presents relevant information.\n",
    "\n",
    "### Why This Matters: The Encoder-Generator Distinction\n",
    "\n",
    "A critical point that even experienced practitioners sometimes misunderstand: the encoder model used for embeddings is completely separate from the generation model that produces the final answer. They serve different purposes and aren't even the same type of model.\n",
    "\n",
    "The encoder (embedding model) is used only during the retrieval step. It converts your question and your documents into vectors so you can find relevant matches. The encoder never sees the full prompt, doesn't generate any text, and doesn't need to be particularly large or powerful—it just needs to create good semantic representations.\n",
    "\n",
    "The generator (autoregressive language model) receives plain text as input, not vectors. It sees the user's question and the retrieved documents as regular language, processes them with its language understanding capabilities, and generates a text response. The generator never sees or works with the vectors themselves.\n",
    "\n",
    "This separation is elegant and powerful. You can use a small, fast encoder for efficient retrieval and a large, capable generator for high-quality responses. You can switch out either component independently. You can even use multiple encoders for different parts of your knowledge base or multiple generators for different types of questions.\n",
    "\n",
    "## Practical Considerations and Limitations\n",
    "\n",
    "As you work with RAG systems, it's important to maintain realistic expectations about what this technique can and cannot do.\n",
    "\n",
    "### RAG as Practical Engineering\n",
    "\n",
    "RAG is, fundamentally, a pragmatic engineering solution rather than a theoretically perfect approach. You might notice that the entire pipeline feels somewhat like a workaround: we're using embeddings to guess what information might be relevant, hoping our retrieval is good enough, and relying on the language model to make sense of whatever we've thrown into its context.\n",
    "\n",
    "And you would be absolutely right to think this. RAG is exactly that—a collection of heuristics and empirical techniques that happen to work reasonably well in practice. There's no mathematical proof that this approach will always retrieve the right information or that the language model will always use the retrieved information appropriately.\n",
    "\n",
    "This empirical nature means that building effective RAG systems requires experimentation and iteration. You'll need to try different embedding models, experiment with chunk sizes and overlap, tune your retrieval parameters, and test your system against realistic queries. What works well for one domain or type of content might work poorly for another.\n",
    "\n",
    "### The Challenge of Retrieval Quality\n",
    "\n",
    "The quality of your RAG system's answers depends critically on the quality of retrieval. If the vector search doesn't find the right documents, the language model has no choice but to either admit it doesn't know or attempt to answer based on its training data (which might lead to hallucinations or incorrect information).\n",
    "\n",
    "This is why significant effort in RAG system development goes into optimizing the retrieval step. **Techniques like hybrid search (combining semantic and keyword-based retrieval), re-ranking retrieved documents, using query expansion, and adjusting chunk sizes** all aim to improve the likelihood that the most relevant information makes it into the final prompt.\n",
    "\n",
    "### Context Window Constraints\n",
    "\n",
    "Even with perfect retrieval, you're constrained by the language model's context window. You can only include so much retrieved information before you either exceed the context limit or dilute the relevance of what you've included. This means you often face trade-offs: retrieve more documents to ensure you have the right information, or retrieve fewer documents to keep the context focused and costs down.\n",
    "\n",
    "### The Evolving Landscape\n",
    "\n",
    "It's worth noting that RAG exists partly as a response to limitations in current language model architectures. As context windows expand (we've gone from 4K tokens to 128K, 200K, or even longer contexts in recent models), some use cases that previously required RAG might be handled by simply including all relevant information directly in the prompt.\n",
    "\n",
    "However, this doesn't make RAG obsolete. Even with infinite context windows, RAG remains valuable for working with truly large knowledge bases, for reducing latency (you don't want to process millions of tokens for every query), for controlling costs, and for maintaining separation between your updatable knowledge base and your relatively static language model.\n",
    "\n",
    "## Looking Forward\n",
    "\n",
    "You now understand the conceptual foundation of RAG: the small idea of including relevant context in prompts, and the big idea of using vector embeddings to find that relevant context based on semantic similarity rather than exact text matching.\n",
    "\n",
    "In upcoming notebook, you'll see this theory transformed into practice. You'll work with embedding models to create vectors, use vector databases to store and query those embeddings, and build complete RAG pipelines using frameworks that handle much of the complexity for you. Tools like LangChain provide abstractions that make RAG development more straightforward, and vector stores like Chroma offer persistent, efficient storage for your embeddings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
