{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd4a002",
   "metadata": {},
   "source": [
    "# Image & Diagram Processing with Vision LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a449866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import fitz\n",
    "from typing import List, Dict\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def extract_images_from_pdf(pdf_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract images from PDF and prepare for vision LLM analysis.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    \n",
    "    for page_num, page in enumerate(doc, start=1):\n",
    "        image_list = page.get_images()\n",
    "        \n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            \n",
    "            pil_image = Image.open(BytesIO(image_bytes))\n",
    "            \n",
    "            # Get surrounding text context\n",
    "            img_rect = page.get_image_rects(xref)[0] if page.get_image_rects(xref) else None\n",
    "            context = \"\"\n",
    "            if img_rect:\n",
    "                # Get text above and below image\n",
    "                context = page.get_text(\"text\", clip=fitz.Rect(\n",
    "                    0, max(0, img_rect.y0 - 100),\n",
    "                    page.rect.width, min(page.rect.height, img_rect.y1 + 100)\n",
    "                ))\n",
    "            \n",
    "            images.append({\n",
    "                'page': page_num,\n",
    "                'image_index': img_index,\n",
    "                'image': pil_image,\n",
    "                'image_bytes': image_bytes,\n",
    "                'context': context.strip(),\n",
    "                'format': base_image['ext'],\n",
    "                'width': base_image['width'],\n",
    "                'height': base_image['height']\n",
    "            })\n",
    "    \n",
    "    doc.close()\n",
    "    return images\n",
    "\n",
    "def analyze_image_with_vision_llm(image_data: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Use GPT-4 Vision to describe images/diagrams for RAG.\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "    \n",
    "    client = OpenAI()\n",
    "    \n",
    "    # Convert PIL image to base64\n",
    "    buffered = BytesIO()\n",
    "    image_data['image'].save(buffered, format=\"PNG\")\n",
    "    img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "    \n",
    "    # Create vision prompt with context\n",
    "    prompt = f\"\"\"Analyze this image from a document (page {image_data['page']}).\n",
    "\n",
    "Surrounding text context:\n",
    "{image_data['context'][:300]}\n",
    "\n",
    "Please provide:\n",
    "1. A concise description of what the image shows\n",
    "2. Key information or data points visible\n",
    "3. How this image relates to the surrounding text\n",
    "4. Any text visible in the image (if applicable)\n",
    "\n",
    "Format your response as plain text suitable for RAG retrieval.\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage for complete image processing pipeline\n",
    "def process_pdf_with_images(pdf_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Complete pipeline: extract images and analyze with vision LLM.\n",
    "    \"\"\"\n",
    "    images = extract_images_from_pdf(pdf_path)\n",
    "    enriched_chunks = []\n",
    "    \n",
    "    for img_data in images:\n",
    "        # Analyze image with vision LLM\n",
    "        description = analyze_image_with_vision_llm(img_data)\n",
    "        \n",
    "        # Create chunk combining context + image description\n",
    "        chunk_text = f\"\"\"\n",
    "[Image on page {img_data['page']}]\n",
    "\n",
    "Context: {img_data['context']}\n",
    "\n",
    "Image Description: {description}\n",
    "\"\"\"\n",
    "        \n",
    "        enriched_chunks.append({\n",
    "            'text': chunk_text.strip(),\n",
    "            'metadata': {\n",
    "                'type': 'image',\n",
    "                'page': img_data['page'],\n",
    "                'image_index': img_data['image_index'],\n",
    "                'has_vision_analysis': True\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return enriched_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e06b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_chunks = process_pdf_with_images(\"RAG_BENCHMARK.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdfae0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[Image on page 6]\n",
       "\n",
       "Context: ACME CORPORATION â€” INTERNAL USE ONLY\n",
       "Certain dependencies introduced latency that could not be isolated to a single functional unit.\n",
       "Operational throughput exhibited non-linear adjustments over the observation window, influenced by\n",
       "regional scheduling constraints. Internal coordination benefited from informal escalation paths that were\n",
       "not formally documented.\n",
       "\n",
       "Image Description: 1. The image shows a step chart with several increases and horizontal segments.\n",
       "\n",
       "2. Key information or data points:\n",
       "   - Several discrete steps representing changes or increments.\n",
       "   - Periods of stability indicated by horizontal lines.\n",
       "\n",
       "3. The image likely illustrates non-linear adjustments in operational throughput as mentioned in the surrounding text. The steps could represent changes influenced by regional scheduling constraints or latency issues.\n",
       "\n",
       "4. Visible text in the image:\n",
       "   - \"Figure 3\" in the top left corner."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(image_chunks[4][\"text\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
