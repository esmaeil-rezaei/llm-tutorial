{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac992bb4",
   "metadata": {},
   "source": [
    "# Creating Interactive Interfaces for LLM Applications\n",
    "\n",
    "Building user interfaces traditionally requires frontend development expertise—knowledge of HTML, CSS, JavaScript, and frameworks like React. For data scientists and backend developers, this creates a significant barrier to demonstrating work or deploying applications. Gradio eliminates this barrier entirely, allowing you to create polished web interfaces with just a few lines of Python code.\n",
    "\n",
    "## Understanding Gradio\n",
    "\n",
    "Gradio is an open-source Python library created by a company now owned by Hugging Face. It enables you to write simple Python code that automatically generates complete web applications. The magic of how this works—the technical details of server creation and React frontend generation—will be explained after you experience it firsthand. For now, focus on the practical aspects of building interfaces.\n",
    "\n",
    "Gradio has become enormously popular in the data science community precisely because it's designed for data science workflows. It's not the only option—Streamlit offers similar capabilities with a different approach—but Gradio excels at rapid prototyping and creating demo applications with minimal code.\n",
    "\n",
    "The fundamental concept is elegantly simple: you define Python functions that perform your core logic, then tell Gradio how those functions should connect to interface elements. Gradio handles everything else—creating the web server, generating the frontend, managing user interactions, and calling your functions when appropriate.\n",
    "\n",
    "## Your First Gradio Application\n",
    "\n",
    "Consider this minimal example from Gradio's documentation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5affa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return f\"Hello {name}!\"\n",
    "\n",
    "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cbd216",
   "metadata": {},
   "source": [
    "This creates a complete web application with an input field, a submit button, and an output display. When users type text and click submit, the `greet` function executes and the result appears in the output area. The entire interface—server, frontend, event handling—comes from these four lines of code.\n",
    "\n",
    "The pattern here represents the core Gradio workflow: define your function, specify inputs and outputs, create an interface, and launch it. Everything else builds on this foundation.\n",
    "\n",
    "## Building a Simple Text Transformer\n",
    "\n",
    "Let's create a practical example. Start by defining a straightforward function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd223724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shout(text):\n",
    "    print(f\"Shout has been called with input: {text}\")\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847bac10",
   "metadata": {},
   "source": [
    "This function takes text as input, prints a confirmation message, and returns the uppercase version. Nothing about this function knows or cares about user interfaces—it's pure Python logic.\n",
    "\n",
    "Now create a Gradio interface for this function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbf3b9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=shout,\n",
    "    inputs=\"textbox\",\n",
    "    outputs=\"textbox\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9382b8fe",
   "metadata": {},
   "source": [
    "When you run this code, a complete web interface appears. You can type text into an input field, click submit, and see the uppercased result in the output field. The console shows the print statement confirming the function was called.\n",
    "\n",
    "The `flagging_mode=\"never\"` parameter disables a special Gradio feature designed for data science workflows where you might want to flag certain results for review. For most applications, you won't need this feature.\n",
    "\n",
    "## Understanding the Callback Pattern\n",
    "\n",
    "The critical concept here is the callback pattern. You're not calling the `shout` function directly. Instead, you're passing the function itself (not the result of calling it) to Gradio:\n",
    "\n",
    "```python\n",
    "fn=shout  # Correct: passing the function\n",
    "# NOT: fn=shout(\"hello\")  # Incorrect: passing a function result\n",
    "```\n",
    "\n",
    "This distinction is fundamental. You're telling Gradio: \"Here's a function you can call whenever you need to.\" Gradio stores this function and calls it (calls back to it) when users interact with the interface. This is why it's called a callback—Gradio calls back to your code at the appropriate time.\n",
    "\n",
    "## Accessing Your Application\n",
    "\n",
    "When you launch a Gradio app, it starts a web server on your local machine. You'll see output indicating the server is running, including a URL like `http://127.0.0.1:7860`. This is a local URL accessible only from your computer.\n",
    "\n",
    "The interface appears embedded directly in your notebook, but clicking the URL opens it in a new browser tab. Both views connect to the same server and function identically. The port number (7860 in this example) increments each time you launch a new Gradio app, as each instance needs its own port.\n",
    "\n",
    "## Sharing Your Application\n",
    "\n",
    "Gradio includes a remarkable feature for sharing applications with others. Adding `share=True` to your launch call does something technically sophisticated:\n",
    "\n",
    "```python\n",
    "demo.launch(share=True)\n",
    "```\n",
    "\n",
    "This uploads your application to Gradio's servers and creates a public URL (ending in `.gradio.live`) that anyone can access. The truly impressive part: when someone uses this public interface, Gradio tunnels the request back to your local machine to execute your function.\n",
    "\n",
    "This works through HTTP tunneling technology (similar to tools like ngrok) that's well-established in technical communities. However, if you work in a corporate environment with strict security policies, this feature might be blocked or could flag security monitoring systems. Use it only in environments where you're certain it's acceptable.\n",
    "\n",
    "**The shared URL remains active for one week and only works while your computer runs the Gradio application**. This makes it perfect for quick demos but isn't suitable for permanent deployment.\n",
    "\n",
    "For production deployment, Gradio provides a `gradio deploy` command that properly hosts your application. This approach is covered in more advanced courses focused on deployment strategies.\n",
    "\n",
    "## Authentication and Security\n",
    "\n",
    "Adding basic authentication to your Gradio app requires just one additional parameter:\n",
    "\n",
    "```python\n",
    "demo.launch(auth=(\"username\", \"password\"))\n",
    "```\n",
    "\n",
    "Users must enter this username and password before accessing the interface. For multiple users, pass a list of tuples:\n",
    "\n",
    "```python\n",
    "demo.launch(auth=[(\"alice\", \"secret1\"), (\"bob\", \"secret2\")])\n",
    "```\n",
    "\n",
    "This provides rudimentary security for shared applications. However, storing passwords in plain text is poor practice. **At minimum, use environment variables to store credentials**. For production applications, implement proper authentication with hashed passwords and secure credential storage.\n",
    "\n",
    "## Customizing Appearance\n",
    "\n",
    "Gradio respects your system's light or dark mode preference by default. While Gradio recommends maintaining this for accessibility, you can force a specific theme if needed:\n",
    "\n",
    "```python\n",
    "demo.launch(js=\"dark\")  # Forces dark mode\n",
    "demo.launch(js=\"light\")  # Forces light mode\n",
    "```\n",
    "\n",
    "The `in_browser=True` parameter automatically opens a browser window when launching, convenient when you always work in a browser rather than the notebook:\n",
    "\n",
    "```python\n",
    "demo.launch(in_browser=True)\n",
    "```\n",
    "\n",
    "## Creating Detailed Interfaces\n",
    "\n",
    "Instead of accepting default settings, you can explicitly define interface components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd4a6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_input = gr.Textbox(\n",
    "    label=\"Your Message\",\n",
    "    placeholder=\"Enter a message to be shouted\",\n",
    "    lines=7\n",
    ")\n",
    "\n",
    "message_output = gr.Textbox(\n",
    "    label=\"Response\",\n",
    "    lines=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fa4ba9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=shout,\n",
    "    title=\"Shout\",\n",
    "    inputs=message_input,\n",
    "    outputs=message_output,\n",
    "    examples=[\"hello\", \"howdy\"],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86260d",
   "metadata": {},
   "source": [
    "This creates a more polished interface with:\n",
    "\n",
    "- Custom labels and placeholders\n",
    "- Specified text area heights\n",
    "- A title for the application\n",
    "- Example inputs users can click to populate the input field\n",
    "\n",
    "The examples feature is particularly useful—users can click an example to instantly populate the input field, making your application more discoverable and easier to use.\n",
    "\n",
    "## Connecting LLMs to Gradio\n",
    "\n",
    "The true power of Gradio emerges when you connect it to language models. Remember the function created earlier for calling GPT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee258eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "def message_gpt(prompt):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d71e0ea",
   "metadata": {},
   "source": [
    "This function takes a prompt, calls GPT, and returns the response. It has the same signature as the `shout` function—one input, one output. This means you can use it with Gradio exactly the same way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43d53aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=message_gpt,\n",
    "    title=\"GPT\",\n",
    "    inputs=gr.Textbox(label=\"Your message\", placeholder=\"Enter a message for GPT\"),\n",
    "    outputs=gr.Textbox(label=\"Response\"),\n",
    "    examples=[\"hello\", \"What is machine learning?\"],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d55de2f",
   "metadata": {},
   "source": [
    "Run this code and you have a complete interface for chatting with GPT. Gradio has no idea it's calling an LLM—from its perspective, this is just another callback function. It handles the interface, you handle the AI logic.\n",
    "\n",
    "## Working with Markdown\n",
    "\n",
    "Language models often produce better output when instructed to use Markdown formatting. Modify your system message:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d877b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant that responds in Markdown without code blocks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38dd33a",
   "metadata": {},
   "source": [
    "Then change the output component to render Markdown:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d02c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=gr.Markdown(label=\"Response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cbdb9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=message_gpt,\n",
    "    title=\"GPT\",\n",
    "    inputs=gr.Textbox(label=\"Your message\", placeholder=\"Enter a message for GPT\"),\n",
    "    outputs=gr.Textbox(label=\"Response\"),\n",
    "    examples=[\"hello\", \"What is machine learning?\"],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801df233",
   "metadata": {},
   "source": [
    "Now responses display with proper formatting—bold text, headings, lists, and other Markdown features render correctly. The instruction to avoid code blocks prevents the model from wrapping its entire response in triple backticks, which would interfere with proper rendering.\n",
    "\n",
    "Test this with different prompts:\n",
    "\n",
    "- \"Explain the transformer architecture to a layperson\"\n",
    "- \"Explain the transformer architecture to an aspiring AI engineer\"\n",
    "\n",
    "The formatted responses demonstrate how Markdown improves readability significantly compared to plain text.\n",
    "\n",
    "## Implementing Streaming Responses\n",
    "\n",
    "When generating longer content, waiting for the complete response before displaying anything creates poor user experience. Streaming shows content as it's generated, creating the familiar typewriter effect.\n",
    "\n",
    "Gradio handles streaming elegantly through Python generators. Create a generator function that yields progressively complete responses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b682e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            result += chunk.choices[0].delta.content\n",
    "            yield result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854deea2",
   "metadata": {},
   "source": [
    "The key differences: `stream=True` in the API call returns a stream object rather than a complete response. Iterating over this stream gives you chunks of content. Each chunk gets added to the accumulated result, and you yield the full accumulated text (not just the new chunk).\n",
    "\n",
    "Gradio recognizes generator functions automatically. When your callback is a generator, Gradio repeatedly calls it and updates the display with each yielded value. This creates the streaming effect without any additional configuration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4054e2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=stream_gpt,  # Generator function\n",
    "    title=\"GPT (Streaming)\",\n",
    "    inputs=gr.Textbox(label=\"Your message\"),\n",
    "    outputs=gr.Markdown(label=\"Response\"),\n",
    "    examples=[\"Explain quantum computing\", \"What is machine learning?\"],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03e36c",
   "metadata": {},
   "source": [
    "The interface looks identical, but now responses stream in progressively rather than appearing all at once.\n",
    "\n",
    "## Supporting Multiple Models\n",
    "\n",
    "Create a single interface that routes to different models based on user selection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5215273f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b924e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(prompt, model):\n",
    "    if model == \"GPT\":\n",
    "        result = stream_gpt(prompt)\n",
    "    elif model == \"Claude\":\n",
    "        # result = stream_claude(prompt)    # Since we don't have a claude api, for running the model we again use openai\n",
    "        result = stream_gpt(prompt)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model}\")\n",
    "\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b1f8b",
   "metadata": {},
   "source": [
    "The `yield from result` statement is shorthand for `for item in result: yield item`—it passes through all values from the nested generator.\n",
    "\n",
    "Now create an interface with multiple inputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0731ef1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_input = gr.Textbox(\n",
    "    label=\"Your message\",\n",
    "    placeholder=\"Enter a message for the LLM\"\n",
    ")\n",
    "\n",
    "model_selector = gr.Dropdown(\n",
    "    choices=[\"GPT\", \"Claude\"],\n",
    "    value=\"GPT\",\n",
    "    label=\"Select Model\"\n",
    ")\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=stream_model,\n",
    "    title=\"LLMs\",\n",
    "    inputs=[message_input, model_selector],\n",
    "    outputs=gr.Markdown(label=\"Response\"),\n",
    "    examples=[\n",
    "        [\"Explain transformers to a layperson\", \"GPT\"],\n",
    "        [\"Explain transformers to an AI engineer\", \"Claude\"]\n",
    "    ],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e4a1d",
   "metadata": {},
   "source": [
    "Notice that inputs is now a list containing both the textbox and dropdown. Examples must also be lists of lists—each example provides values for both inputs.\n",
    "\n",
    "Users can now select their preferred model from a dropdown before submitting their question. The same interface seamlessly routes to different backend services based on user choice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
