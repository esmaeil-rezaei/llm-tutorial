{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed06bc40",
   "metadata": {},
   "source": [
    "# Building a Voice-Enabled AI Assistant with Gradio\n",
    "\n",
    "This code creates an interactive AI assistant that can both speak and listen. Users can type messages or record their voice, and the AI responds with natural-sounding speech.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a17816",
   "metadata": {},
   "source": [
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    USER INPUT                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "â”‚  â”‚ Type Text    â”‚      OR      â”‚ Speak/Upload â”‚         â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â”‚         â”‚                             â”‚                 â”‚\n",
    "â”‚         â”‚                             â–¼                 â”‚\n",
    "â”‚         â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "â”‚         â”‚                    â”‚ Whisper STT    â”‚         â”‚\n",
    "â”‚         â”‚                    â”‚ (voiceâ†’text)   â”‚         â”‚\n",
    "â”‚         â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â”‚                        â–¼                                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚   Unified Text Input   â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              CONVERSATION MEMORY                        â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\n",
    "â”‚  â”‚ History = [                              â”‚           â”‚\n",
    "â”‚  â”‚   {role: \"user\", content: \"Hi\"},         â”‚           â”‚\n",
    "â”‚  â”‚   {role: \"assistant\", content: \"Hello!\"}, â”‚          â”‚\n",
    "â”‚  â”‚   ... previous messages ...              â”‚           â”‚\n",
    "â”‚  â”‚ ]                                        â”‚           â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\n",
    "â”‚                     â”‚                                   â”‚\n",
    "â”‚                     â–¼                                   â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\n",
    "â”‚  â”‚ Add new user message to history          â”‚           â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚   Send to GPT-4o-mini     â”‚\n",
    "         â”‚   (entire conversation)   â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚   AI generates response   â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚   Add AI response to      â”‚\n",
    "         â”‚   conversation history    â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   OUTPUT                              â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚         â”‚  Display text response  â”‚                   â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚                     â”‚                                 â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚         â”‚  Convert to speech      â”‚                   â”‚\n",
    "â”‚         â”‚  (OpenAI TTS API)       â”‚                   â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚                     â”‚                                 â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚         â”‚  Play audio (pygame)    â”‚                   â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71268d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed audio: Hi, how are you? How's it going?\n",
      "Transcribed audio: I want to purchase a product, so could you please help me with that?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "import pygame\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "pygame.mixer.init()\n",
    "\n",
    "def speak(text, voice=\"alloy\"):\n",
    "    try:\n",
    "        speech_file = os.path.join('saved audio', f\"speech_{datetime.now().timestamp()}.mp3\")\n",
    "        \n",
    "        with client.audio.speech.with_streaming_response.create(\n",
    "            model=\"tts-1\",\n",
    "            voice=voice,\n",
    "            input=text\n",
    "        ) as response:\n",
    "            response.stream_to_file(speech_file)\n",
    "        \n",
    "        pygame.mixer.music.load(speech_file)\n",
    "        pygame.mixer.music.play()\n",
    "        \n",
    "        try:\n",
    "            while pygame.mixer.music.get_busy():\n",
    "                pygame.time.Clock().tick(10)\n",
    "        except KeyboardInterrupt:\n",
    "            pygame.mixer.music.stop()\n",
    "        \n",
    "        os.remove(speech_file)\n",
    "    except Exception as e:\n",
    "        print(f\"TTS Error: {e}\")\n",
    "\n",
    "# Whisper Transcription\n",
    "def transcribe_audio(audio_file):\n",
    "    try:\n",
    "        with open(audio_file, \"rb\") as f:\n",
    "            transcript = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",\n",
    "                file=f\n",
    "            )\n",
    "        return transcript.text\n",
    "    except Exception as e:\n",
    "        print(f\"Transcription error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Chat Interface\n",
    "def chat_interface(text_input, chat_history, voice_choice, audio_file=None):\n",
    "    \n",
    "    # Get user input from text or audio\n",
    "    user_input = text_input\n",
    "    if audio_file:\n",
    "        user_input = transcribe_audio(audio_file)\n",
    "        print(f\"Transcribed audio: {user_input}\")\n",
    "\n",
    "    if not user_input or not user_input.strip():\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    # Initialize chat history if None\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful, natural AI assistant.\"}]\n",
    "    \n",
    "    # Handle both old list format [[user, ai]] and new dict format\n",
    "    for msg in chat_history:\n",
    "        if isinstance(msg, dict):\n",
    "            # New Gradio format\n",
    "            messages.append(msg)\n",
    "        elif isinstance(msg, (list, tuple)) and len(msg) == 2:\n",
    "            # Old Gradio format - convert to OpenAI format\n",
    "            messages.append({\"role\": \"user\", \"content\": msg[0]})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": msg[1]})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Get AI response\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        ai_reply = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        ai_reply = f\"Error: {e}\"\n",
    "\n",
    "    # Speak the response\n",
    "    speak(ai_reply, voice_choice)\n",
    "\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": ai_reply})\n",
    "\n",
    "    return chat_history, \"\"\n",
    "\n",
    "\n",
    "# Gradio App\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ğŸ™ï¸ Human-Like AI Voice Assistant\")\n",
    "    gr.Markdown(\"Type or speak your message, and the AI will respond with voice!\")\n",
    "    \n",
    "    chatbot = gr.Chatbot()\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            txt = gr.Textbox(\n",
    "                placeholder=\"Type your message here...\", \n",
    "                label=\"Text Input\",\n",
    "                lines=2\n",
    "            )\n",
    "        with gr.Column(scale=1):\n",
    "            voice = gr.Dropdown(\n",
    "                [\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"], \n",
    "                value=\"alloy\", \n",
    "                label=\"Voice\"\n",
    "            )\n",
    "    \n",
    "    audio_input = gr.Audio(label=\"Or record/upload audio\", type=\"filepath\")\n",
    "    \n",
    "    send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
    "    \n",
    "    send_btn.click(\n",
    "        chat_interface, \n",
    "        inputs=[txt, chatbot, voice, audio_input], \n",
    "        outputs=[chatbot, txt]\n",
    "    )\n",
    "    \n",
    "    txt.submit(\n",
    "        chat_interface, \n",
    "        inputs=[txt, chatbot, voice, audio_input], \n",
    "        outputs=[chatbot, txt]\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1297de5",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">> **IMPORTANT NOTE**: Run the local url to use the audio feature</span>\n",
    "\n",
    "This guide breaks down how each part works and why it's structured this way.\n",
    "\n",
    "# Part One: Setting Up Your Tools\n",
    "\n",
    "Before building the interface, you need to import the tools your application will use and initialize the services that power your assistant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a828c7a",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "import pygame\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "pygame.mixer.init()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43dcbf",
   "metadata": {},
   "source": [
    "# Part Two: Teaching Your Assistant to Speak\n",
    "\n",
    "The first major function converts text into spoken audio using OpenAI's text-to-speech (TTS) service.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647417f",
   "metadata": {},
   "source": [
    "```python\n",
    "def speak(text, voice=\"alloy\"):\n",
    "    try:\n",
    "        speech_file = os.path.join('saved audio', f\"speech_{datetime.now().timestamp()}.mp3\")\n",
    "\n",
    "        with client.audio.speech.with_streaming_response.create(\n",
    "            model=\"tts-1\",\n",
    "            voice=voice,\n",
    "            input=text\n",
    "        ) as response:\n",
    "            response.stream_to_file(speech_file)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2aba1c",
   "metadata": {},
   "source": [
    "### Breaking it down\n",
    "\n",
    "- The function takes two parameters: `text` (what to say) and `voice` (which voice to use), with `\"alloy\"` set as the default voice.\n",
    "- It creates a unique filename using the current timestamp and saves the audio file inside a dedicated _saved audio_ folder.\n",
    "- The `with` statement requests OpenAI to generate speech and uses streaming to efficiently handle the incoming audio data.\n",
    "- The `response.stream_to_file()` method writes the generated speech directly to an MP3 file on disk.\n",
    "\n",
    "The `client` object represents your connection to OpenAIâ€™s API. It automatically reads the API key from an environment variable, which is a secure and recommended way to handle sensitive credentials without hard-coding them into your source code. The `pygame.mixer.init()` line initializes the audio subsystem, making sure your application is ready to load and play sound files such as generated speech.\n",
    "\n",
    "### Part Two: Teaching Your Assistant to Speak\n",
    "\n",
    "The first major function focuses on converting text into spoken audio using OpenAIâ€™s text-to-speech (TTS) service. This function sends a text prompt to the TTS model, receives synthesized speech as audio data, and saves it as a playable sound file. Once generated, the audio can be played back immediately, allowing the assistant to respond in a natural, human-like voice rather than plain text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de83223",
   "metadata": {},
   "source": [
    "```python\n",
    "def speak(text, voice=\"alloy\"):\n",
    "    try:\n",
    "        speech_file = os.path.join('saved audio', f\"speech_{datetime.now().timestamp()}.mp3\")\n",
    "\n",
    "        with client.audio.speech.with_streaming_response.create(\n",
    "            model=\"tts-1\",\n",
    "            voice=voice,\n",
    "            input=text\n",
    "        ) as response:\n",
    "            response.stream_to_file(speech_file)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64546f",
   "metadata": {},
   "source": [
    "### Breaking It Down\n",
    "\n",
    "The function accepts two parameters: `text`, which represents the content that will be spoken, and `voice`, which specifies the voice style to use. If no voice is provided, it defaults to `\"alloy\"`, ensuring consistent behavior without requiring extra input from the user.\n",
    "\n",
    "To avoid filename conflicts, the function generates a unique filename using the current timestamp and saves the output inside a dedicated â€œsaved audioâ€ folder. This makes each generated audio file distinct and easy to manage or replay later if needed.\n",
    "\n",
    "The `with` statement is used to request speech generation from OpenAI while enabling streaming. Streaming allows the audio data to be processed efficiently as it is generated, rather than waiting for the entire file to be created at once. Finally, `response.stream_to_file()` writes the streamed audio data directly to an MP3 file, producing a ready-to-play speech output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba89f92f",
   "metadata": {},
   "source": [
    "```python\n",
    "        pygame.mixer.music.load(speech_file)\n",
    "        pygame.mixer.music.play()\n",
    "\n",
    "        try:\n",
    "            while pygame.mixer.music.get_busy():\n",
    "                pygame.time.Clock().tick(10)\n",
    "        except KeyboardInterrupt:\n",
    "            pygame.mixer.music.stop()\n",
    "\n",
    "        os.remove(speech_file)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a3ba5",
   "metadata": {},
   "source": [
    "### What Happens Next\n",
    "\n",
    "Once the audio file is generated, `load()` prepares it for playback by loading it into the audio systemâ€™s memory. Calling `play()` then starts the audio output, allowing the assistantâ€™s response to be heard through the speakers. A `while` loop continuously checks whether the audio is still playing, pausing briefly (every 10 milliseconds) to keep the program responsive without consuming unnecessary CPU resources.\n",
    "\n",
    "If you press **Ctrl+C** during playback, the program immediately stops, giving you manual control to interrupt the audio at any time. After the playback finishes (or is interrupted), `os.remove()` deletes the temporary audio file, ensuring disk space is not wasted by leftover files.\n",
    "\n",
    "The entire function is wrapped in a `try/except` block, which allows any unexpected errorsâ€”such as missing audio devices or file issuesâ€”to be handled gracefully. Instead of crashing the application, the function prints a clear error message and continues running.\n",
    "\n",
    "### Part Three: Teaching Your Assistant to Listen\n",
    "\n",
    "This next function focuses on the opposite direction of communication: turning spoken input into text. It records audio from the microphone and sends it to OpenAIâ€™s **Whisper** speech recognition model, which transcribes the audio into readable text. This transcription is then passed to the assistant as user input, enabling natural, voice-driven conversations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d56e6",
   "metadata": {},
   "source": [
    "```python\n",
    "def transcribe_audio(audio_file):\n",
    "    try:\n",
    "        with open(audio_file, \"rb\") as f:\n",
    "            transcript = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",\n",
    "                file=f\n",
    "            )\n",
    "        return transcript.text\n",
    "    except Exception as e:\n",
    "        print(f\"Transcription error: {e}\")\n",
    "        return \"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6857708",
   "metadata": {},
   "source": [
    "### How It Works\n",
    "\n",
    "The transcription function starts by taking the path to an audio file as its input. Because audio files are binary data rather than plain text, the file is opened in **read-binary (`\"rb\"`) mode**, which ensures the raw audio bytes are handled correctly. This audio data is then sent to OpenAIâ€™s **Whisper** speech recognition model, which analyzes the sound and converts spoken words into text. Once the transcription is complete, the function returns the recognized text so it can be used as normal user input.\n",
    "\n",
    "To keep the application stable, the function is wrapped in error handling logic. If anything goes wrongâ€”such as a missing file, microphone issue, or API errorâ€”it safely returns an empty string instead of crashing the entire program. Compared to the text-to-speech function, this process is simpler because it only sends data to OpenAI and waits for text in return, without needing to manage audio playback or file cleanup.\n",
    "\n",
    "### Part Four: Orchestrating the Conversation\n",
    "\n",
    "The `chat_interface` function acts as the **brain of the assistant**. It ties all the individual pieces together by deciding how user input is handled, whether that input comes from typed text or transcribed speech. Once the input is ready, it sends the messageâ€”along with conversation historyâ€”to the language model to generate a context-aware response.\n",
    "\n",
    "After receiving the AIâ€™s reply, the function not only displays it but also passes it to the text-to-speech system so the assistant can respond out loud. In this way, `chat_interface` coordinates listening, thinking, and speaking, turning separate capabilities into a smooth, human-like conversational experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12908c",
   "metadata": {},
   "source": [
    "```python\n",
    "def chat_interface(text_input, chat_history, voice_choice, audio_file=None):\n",
    "    # Get user input from text or audio\n",
    "    user_input = text_input\n",
    "    if audio_file:\n",
    "        user_input = transcribe_audio(audio_file)\n",
    "        print(f\"Transcribed audio: {user_input}\")\n",
    "\n",
    "    if not user_input or not user_input.strip():\n",
    "        return chat_history, \"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3763407b",
   "metadata": {},
   "source": [
    "### Understanding the Parameters\n",
    "\n",
    "The function is designed to handle both text-based and voice-based interactions, so it accepts several inputs that cover all possible user actions. The `text_input` parameter contains whatever the user typed into the chat box, if any. The `chat_history` parameter stores the full conversation so far, allowing the assistant to respond with proper context instead of treating each message in isolation. The `voice_choice` parameter controls which synthesized voice the assistant will use when speaking its response. Finally, `audio_file` is optional and represents a recorded voice message from the user, which may need to be transcribed before processing.\n",
    "\n",
    "When the function runs, its first responsibility is to figure out **what the user actually said**. If an audio file is provided, the function transcribes it into text; otherwise, it falls back to the typed input. This unified text representation ensures the rest of the logic works the same way regardless of how the user communicated. If the resulting input is empty or contains only whitespace, the function exits immediately without calling the language model, preventing unnecessary API calls and keeping the chatbot responsive and efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f950caec",
   "metadata": {},
   "source": [
    "```python\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful, natural AI assistant.\"}]\n",
    "\n",
    "    for msg in chat_history:\n",
    "        if isinstance(msg, dict):\n",
    "            messages.append(msg)\n",
    "        elif isinstance(msg, (list, tuple)) and len(msg) == 2:\n",
    "            messages.append({\"role\": \"user\", \"content\": msg[0]})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": msg[1]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e68eaad",
   "metadata": {},
   "source": [
    "### Building the Conversation Context\n",
    "\n",
    "The function begins by ensuring there is a conversation history to work with. If this is the userâ€™s first message, an empty history is initialized so the assistant has a clean starting point. A new `messages` list is then created, beginning with a **system message** that defines the assistantâ€™s role, tone, and behavior. This system message acts as the â€œpersonality anchorâ€ for the AI and is always included at the start of the conversation sent to OpenAI.\n",
    "\n",
    "Next, the function loops through the existing chat history and converts each turn into the format expected by OpenAI. This step is necessary because different versions of Gradio may store conversation data differentlyâ€”sometimes as dictionaries and sometimes as lists or tuples. The function handles both cases defensively, extracting the user and assistant messages and transforming them into dictionaries with explicit `\"role\"` and `\"content\"` keys. Finally, the userâ€™s new message is appended to the end of the `messages` list, ensuring the model sees the full, correctly structured conversation context before generating its response.\n",
    "\n",
    "This conversion layer is crucial because Gradio and OpenAI represent conversations differently. OpenAIâ€™s API requires a strict message schema, while Gradio prioritizes UI flexibility. Bridging this gap ensures the assistant remains coherent, context-aware, and compatible across different frontend implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e427f1",
   "metadata": {},
   "source": [
    "```python\n",
    "try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        ai_reply = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        ai_reply = f\"Error: {e}\"\n",
    "\n",
    "    speak(ai_reply, voice_choice)\n",
    "\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": ai_reply})\n",
    "\n",
    "    return chat_history, \"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b5626",
   "metadata": {},
   "source": [
    "### Getting and Delivering the Response\n",
    "\n",
    "Once the conversation context is fully assembled, all messages are sent to OpenAIâ€™s **GPT-4o-mini** model to generate the assistantâ€™s reply. The function then extracts the AIâ€™s text response from the API output, isolating just the content that should be shown and spoken. If anything goes wrong during this processâ€”such as a network issue or API errorâ€”the function gracefully falls back to creating a readable error message instead of crashing the application.\n",
    "\n",
    "After a valid response is obtained, the assistant immediately calls the `speak()` function to convert the text into natural-sounding speech and play it aloud. Both the userâ€™s message and the AIâ€™s response are then appended to the chat history, preserving conversational continuity for future turns. Finally, the function returns the updated chat history along with an empty string, which Gradio interprets as a signal to clear the text input box and prepare the interface for the next message.\n",
    "\n",
    "### Part Five: Building the User Interface\n",
    "\n",
    "With the conversational logic in place, the final step is constructing the visual interface. **Gradio Blocks** provides a flexible layout system that allows you to precisely arrange UI components such as chat windows, text inputs, audio uploaders, and buttons. Unlike simpler Gradio interfaces, Blocks lets you control structure and flow explicitly, making it ideal for building a polished, chatbot-like experience that feels responsive and intuitive to users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4717b2fe",
   "metadata": {},
   "source": [
    "```python\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ğŸ™ï¸ Human-Like AI Voice Assistant\")\n",
    "    gr.Markdown(\"Type or speak your message, and the AI will respond with voice!\")\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4bca71",
   "metadata": {},
   "source": [
    "### Creating the Foundation\n",
    "\n",
    "The application begins inside a `with gr.Blocks()` context, which establishes the overall structure of your Gradio app and allows you to define all interface components in a clean, organized way. Inside this block, the first elements added are formatted text components using Markdown. The `#` symbol creates a large, prominent heading, which is useful for clearly presenting the title or purpose of your assistant to the user.\n",
    "\n",
    "Next, `gr.Chatbot()` is used to create the main conversation display area. This component is responsible for visually rendering the back-and-forth dialogue between the user and the AI, showing messages in a familiar chat-style format. It automatically updates as new messages are added, giving the assistant the look and feel of a real-time conversational agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed5165",
   "metadata": {},
   "source": [
    "```python\n",
    "with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            txt = gr.Textbox(\n",
    "                placeholder=\"Type your message here...\",\n",
    "                label=\"Text Input\",\n",
    "                lines=2\n",
    "            )\n",
    "        with gr.Column(scale=1):\n",
    "            voice = gr.Dropdown(\n",
    "                [\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\n",
    "                value=\"alloy\",\n",
    "                label=\"Voice\"\n",
    "            )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad56f44",
   "metadata": {},
   "source": [
    "### Organizing the Input Controls\n",
    "\n",
    "The input section is organized using `gr.Row()`, which places components horizontally on the same line. Inside this row, two columns are defined to divide the available space in a clear and balanced way. The first column uses a larger scale value, taking roughly four-fifths of the width, while the second column uses a smaller scale, occupying the remaining one-fifth. This proportional layout is controlled by the `scale` parameter, which determines how much horizontal space each column receives relative to the others.\n",
    "\n",
    "The wider column contains the main text input box, set to two lines tall so users can comfortably type short messages or questions. The narrower column holds a dropdown menu that allows the user to select the AIâ€™s voice. Placing the voice selector alongside the text input keeps all interaction controls in one compact area, making the interface intuitive while preserving a clean, chat-like experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e1cb2",
   "metadata": {},
   "source": [
    "```python\n",
    "    audio_input = gr.Audio(label=\"Or record/upload audio\", type=\"filepath\")\n",
    "\n",
    "    send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddbcb2b",
   "metadata": {},
   "source": [
    "### Adding More Controls\n",
    "\n",
    "Additional input controls enhance the interactivity of the chat interface. The `gr.Audio()` component allows users to either record their voice directly or upload an existing audio file. By setting `type=\"filepath\"`, Gradio passes the path to the saved audio file to your backend function, making it easy to process with the AI. This enables seamless voice-based interactions without requiring manual file handling.\n",
    "\n",
    "A `gr.Button()` component acts as the send button for the chat. Using `variant=\"primary\"` makes the button visually prominent, signaling to users that it is the main action to submit their message. Together, the audio input and send button provide multiple ways for users to communicate with the AI, whether by speaking or typing, improving accessibility and user experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d03c4a",
   "metadata": {},
   "source": [
    "```python\n",
    "send_btn.click(\n",
    "        chat_interface,\n",
    "        inputs=[txt, chatbot, voice, audio_input],\n",
    "        outputs=[chatbot, txt]\n",
    "    )\n",
    "\n",
    "    txt.submit(\n",
    "        chat_interface,\n",
    "        inputs=[txt, chatbot, voice, audio_input],\n",
    "        outputs=[chatbot, txt]\n",
    "    )\n",
    "\n",
    "demo.launch()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7136939",
   "metadata": {},
   "source": [
    "### Making It Interactive\n",
    "\n",
    "Interactivity in Gradio is achieved by connecting UI components to backend functions. The `send_btn.click()` method specifies that **when someone clicks the Send button, the `chat_interface` function should be called**. The `inputs` list tells Gradio which components provide data to the functionâ€”such as the text input, chat history, selected voice, or audio fileâ€”while the `outputs` list defines which components get updated with the functionâ€™s return values, typically the chatbot display and the text input box.\n",
    "\n",
    "Similarly, `txt.submit()` allows users to **trigger the same function by pressing Enter** in the text box, enabling a more natural chat experience. Finally, `demo.launch()` starts a local web server and opens the interface in a browser. This setup ensures that whenever a user types a message or records audio, Gradio:\n",
    "\n",
    "1. Collects values from the input components.\n",
    "2. Calls the `chat_interface` function with these values.\n",
    "3. Updates the output components with the AIâ€™s response.\n",
    "4. Refreshes the chat display in real-time.\n",
    "\n",
    "This clear data flow creates a smooth, responsive, and human-like chat interaction.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
